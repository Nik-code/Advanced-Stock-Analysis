================================================================
RepopackPy Output File
================================================================

This file was generated by RepopackPy on: 2024-09-12T09:08:14.614817

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This header section
2. Repository structure
3. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
1. This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
2. When processing this file, use the separators and "File:" markers to
  distinguish between different files in the repository.
3. Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and RepopackPy's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

For more information about RepopackPy, visit: https://github.com/abinthomasonline/repopack-py

================================================================
Repository Structure
================================================================
app/
  api/
    __init__.py
    stocks.py
  db/
    __init__.py
    influx_client.py
    influx_writer.py
  models/
    __init__.py
    arima_model.py
    backtesting.py
    lstm_model.py
    train_and_save_model.py
  services/
    __init__.py
    data_collection.py
    data_fetcher.py
    data_processor.py
    llm_integration.py
    technical_indicators.py
    zerodha_service.py
  __init__.py
scripts/
  arima_strategy.py
  get_stocks_list.py
  model_comparison.py
tests/
  __init__.py
  conftest.py
  query_influxdb.py
  test_api_endpoints.py
  test_historical.py
  test_update_data.py
  test_zerodha_connection.py
  test_zerodha_service.py
__init__.py
main.py
requirements.txt

================================================================
Repository Files
================================================================

================
File: requirements.txt
================
fastapi
uvicorn
pandas
numpy
requests
python-dotenv
motor
apscheduler
bsedata
pytest
httpx
scikit-learn
aiofiles
yfinance
kiteconnect==4.1.0
influxdb-client==1.36.0

================
File: main.py
================
import os

import joblib
import logging
import numpy as np

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from dotenv import find_dotenv, load_dotenv, set_key
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List

from app.api import stocks
from app.models.backtesting import backtest_lstm_model, backtest_arima_model
from app.models.lstm_model import LSTMStockPredictor
from app.services.data_collection import fetch_historical_data, fetch_news_data
from app.services.llm_integration import GPT4Processor
from app.services.technical_indicators import calculate_sma, calculate_ema, calculate_rsi, calculate_macd, calculate_bollinger_bands, calculate_atr
from app.services.zerodha_service import ZerodhaService

load_dotenv()

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()  # This line is crucial

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # Add your frontend URL here
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

scheduler = AsyncIOScheduler()
zerodha_service = ZerodhaService()
llm_processor = GPT4Processor()

app.include_router(stocks.router, prefix="/api")


@app.get("/")
async def root():
    return {"message": "BSE Stock Analysis API is running"}


@app.get("/api/quote")
async def get_quote(instruments: str):
    """
    Fetch the latest market quote for the specified instruments.

    This endpoint retrieves the latest trading information for the given stock instruments.

    Parameters:
    - instruments (str): A comma-separated list of instrument codes (e.g., "BSE:RELIANCE").

    Example of a request:
    http://localhost:8000/api/quote?instruments=BSE:RELIANCE

    Returns:
    - dict: A dictionary containing the latest quote data for the specified instruments.
            The structure of the response is as follows:
            {
                "BSE:RELIANCE": {
                    "instrument_token": 128083204,
                    "timestamp": "2024-09-06T16:01:32",
                    "last_trade_time": "2024-09-06T15:59:57",
                    "last_price": 2929.85,
                    "last_quantity": 0,
                    "buy_quantity": 0,
                    "sell_quantity": 0,
                    "volume": 0,
                    "average_price": 0,
                    "oi": 0,
                    "oi_day_high": 0,
                    "oi_day_low": 0,
                    "net_change": 0,
                    "lower_circuit_limit": 2636.9,
                    "upper_circuit_limit": 3222.8,
                    "ohlc": {
                        "open": 2989.9,
                        "high": 2996.2,
                        "low": 2922.75,
                        "close": 2987.15
                    },
                    "depth": {
                        "buy": [
                            {"price": 0, "quantity": 0, "orders": 0},
                            {"price": 0, "quantity": 0, "orders": 0},
                            {"price": 0, "quantity": 0, "orders": 0},
                            {"price": 0, "quantity": 0, "orders": 0},
                            {"price": 0, "quantity": 0, "orders": 0}
                        ],
                        "sell": [
                            {"price": 0, "quantity": 0, "orders": 0},
                            {"price": 0, "quantity": 0, "orders": 0},
                            {"price": 0, "quantity": 0, "orders": 0},
                            {"price": 0, "quantity": 0, "orders": 0},
                            {"price": 0, "quantity": 0, "orders": 0}
                        ]
                    }
                }
            }

    Raises:
    - HTTPException: If the quote is not found or if an internal error occurs.
    """
    try:
        quote_data = zerodha_service.get_quote(instruments)
        if quote_data is None:
            raise HTTPException(status_code=404, detail="Quote not found")
        return quote_data
    except Exception as e:
        logger.error(f"Error fetching quote for {instruments}: {str(e)}")
        raise HTTPException(status_code=500, detail="Error fetching quote")


@app.get("/api/live/{stock_code}")
async def get_live_stock_data(stock_code: str):
    try:
        instrument_token = zerodha_service.get_instrument_token("BSE", stock_code)
        if not instrument_token:
            raise HTTPException(status_code=404, detail=f"No instrument token found for stock code {stock_code}")
        live_data = zerodha_service.get_quote([instrument_token])
        if live_data is None or str(instrument_token) not in live_data:
            raise HTTPException(status_code=404, detail=f"No live data found for stock code {stock_code}")
        return live_data[str(instrument_token)]
    except Exception as e:
        logger.error(f"Error fetching live data for {stock_code}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/historical/{stock_code}")
async def get_historical_stock_data(stock_code: str, timeframe: str = '1year'):
    try:
        historical_data = await fetch_historical_data(stock_code, timeframe)
        if historical_data is None:
            raise HTTPException(status_code=404, detail=f"No historical data found for stock code {stock_code}")
        return historical_data.to_dict(orient='records')
    except Exception as e:
        logger.error(f"Error fetching historical data for {stock_code}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/news/{stock_code}")
async def get_stock_news(stock_code: str):
    try:
        news_data = await fetch_news_data(stock_code)
        return {"news": news_data}
    except Exception as e:
        logger.error(f"Error fetching news for {stock_code}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/predict/{stock_code}")
async def predict_stock(stock_code: str):
    try:
        historical_data = await fetch_historical_data(stock_code, '1year')
        if historical_data is None:
            raise HTTPException(status_code=404, detail=f"No historical data found for {stock_code}")

        lstm_prediction = await get_lstm_prediction(stock_code, historical_data)
        arima_prediction = await get_arima_prediction(stock_code, historical_data)

        return {
            "lstm_prediction": lstm_prediction,
            "arima_prediction": arima_prediction
        }
    except Exception as e:
        logger.error(f"Error making prediction for {stock_code}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/analysis/{stock_code}")
async def get_stock_analysis(stock_code: str):
    try:
        historical_data = await fetch_historical_data(stock_code, '1year')
        news_data = await fetch_news_data(stock_code)
        lstm_prediction = await get_lstm_prediction(stock_code, historical_data)
        arima_prediction = await get_arima_prediction(stock_code, historical_data)

        analysis = await llm_processor.analyze_stock(
            stock_code,
            historical_data,
            news_data,
            lstm_prediction,
            arima_prediction
        )

        return analysis
    except Exception as e:
        logger.error(f"Error generating analysis for {stock_code}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/top_stocks")
async def get_top_stocks():
    try:
        # Implement logic to analyze all stocks and return top picks
        top_stocks = await analyze_all_stocks()
        return {"top_stocks": top_stocks}
    except Exception as e:
        logger.error(f"Error getting top stocks: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# Scheduled task to run predictions and make trade decisions
@app.on_event("startup")
@repeat_every(seconds=60 * 60 * 8)  # Run every 8 hours
async def run_predictions_and_trades():
    try:
        # Implement logic to run predictions and make trade decisions
        await run_predictions_and_make_trades()
    except Exception as e:
        logger.error(f"Error in scheduled task: {str(e)}")


@app.get("/api/login")
async def login():
    """
    Endpoint to initiate the login process for the Zerodha trading platform.

    This endpoint returns the login URL that the user needs to visit in order to authenticate
    and generate a session token. The user will be redirected to the Zerodha login page,
    where they can enter their credentials.

    Returns:
    - dict: A dictionary containing the login URL.

    Example of a successful response:
    {
        "login_url": "https://zerodha.com/login"
    }
    """
    login_url = zerodha_service.get_login_url()
    return {"login_url": login_url}


@app.get("/api/callback")
async def callback(request: Request):
    """
    Endpoint to handle the callback from the Zerodha login process.

    This endpoint is called after the user has authenticated with Zerodha and is redirected back
    to the application. It retrieves the request token from the query parameters, generates an
    access token, and saves it for future use.

    Parameters:
    - request (Request): The incoming request containing the query parameters.

    Returns:
    - dict: A dictionary containing the access token.

    Raises:
    - HTTPException: If no request token is provided (400 Bad Request).
    - HTTPException: If there is an error generating the session (500 Internal Server Error).

    Example of a successful response:
    {
        "access_token": "your_access_token_here"
    }
    """
    # Log the full request details to check for the request_token
    logger.info(f"Request params: {request.query_params}")

    params = dict(request.query_params)
    request_token = params.get("request_token")

    if not request_token:
        raise HTTPException(status_code=400, detail="No request token provided")

    try:
        access_token = zerodha_service.generate_session(request_token)
        zerodha_service.set_access_token(access_token)
        # Save the access token to .env file
        dotenv_file = find_dotenv()
        set_key(dotenv_file, "ZERODHA_ACCESS_TOKEN", access_token)
        return {"access_token": access_token}
    except Exception as e:
        logger.error(f"Error in callback: {str(e)}")
        raise HTTPException(status_code=500, detail="Error generating session")


@app.get("/api/stocks/{symbol}/indicators")
async def get_technical_indicators(symbol: str, timeFrame: str = '1year'):
    """
    Retrieve technical indicators for a specified stock symbol over a given time frame.

    This endpoint calculates various technical indicators based on historical stock data, including:
    - Simple Moving Average (SMA)
    - Exponential Moving Average (EMA)
    - Relative Strength Index (RSI)
    - Moving Average Convergence Divergence (MACD)
    - Bollinger Bands
    - Average True Range (ATR)

    Parameters:
    - symbol (str): The stock symbol for which to retrieve technical indicators (e.g., "RELIANCE").
    - timeFrame (str): The time frame for historical data (default is '1year').

    Returns:
    - dict: A dictionary containing the calculated technical indicators and historical data, structured as follows:
        {
            "sma_20": List[float],  # 20-period SMA values
            "ema_50": List[float],  # 50-period EMA values
            "rsi_14": List[float],   # 14-period RSI values
            "macd": List[float],     # MACD values
            "macd_signal": List[float],  # MACD signal line values
            "bollinger_upper": List[float],  # Upper Bollinger Band values
            "bollinger_middle": List[float],  # Middle Bollinger Band values
            "bollinger_lower": List[float],  # Lower Bollinger Band values
            "atr": List[float],      # Average True Range values
            "dates": List[str],      # Dates corresponding to the historical data
            "close_prices": List[float]  # Closing prices for the historical data
        }

    Raises:
    - HTTPException: If no historical data is found for the specified stock symbol (404 Not Found).
    - HTTPException: If an error occurs during the calculation of technical indicators (500 Internal Server Error).
    """
    try:
        historical_data = await fetch_historical_data(symbol, timeFrame)
        if historical_data is None:
            raise HTTPException(status_code=404, detail=f"No data found for stock symbol {symbol}")

        logger.info(f"Columns in the dataframe: {historical_data.columns}")
        logger.info(f"Number of data points: {len(historical_data)}")

        # Calculate technical indicators
        sma_20 = calculate_sma(historical_data['close'], 20)
        ema_50 = calculate_ema(historical_data['close'], 50)
        rsi_14 = calculate_rsi(historical_data['close'], 14)
        macd, signal, _ = calculate_macd(historical_data['close'])
        upper, middle, lower = calculate_bollinger_bands(historical_data['close'])
        atr = calculate_atr(historical_data['high'], historical_data['low'], historical_data['close'], 14)

        def safe_float_list(arr):
            return [float(x) if not np.isnan(x) and not np.isinf(x) else None for x in arr]

        return {
            "sma_20": safe_float_list(sma_20),
            "ema_50": safe_float_list(ema_50),
            "rsi_14": safe_float_list(rsi_14),
            "macd": safe_float_list(macd),
            "macd_signal": safe_float_list(signal),
            "bollinger_upper": safe_float_list(upper),
            "bollinger_middle": safe_float_list(middle),
            "bollinger_lower": safe_float_list(lower),
            "atr": safe_float_list(atr),
            "dates": historical_data['date'].tolist(),
            "close_prices": safe_float_list(historical_data['close'])
        }
    except Exception as e:
        logger.error(f"Error calculating technical indicators for {symbol}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/stocks/{symbol}/realtime")
async def get_realtime_data(symbol: str):
    """
    Fetch real-time market data for a specified stock symbol.

    This endpoint retrieves the latest trading information for the given stock symbol, including
    instrument token, last price, volume, and order book depth.

    Parameters:
    - symbol (str): The stock symbol for which to fetch real-time data (e.g., "RELIANCE").

    Returns:
    - dict: A dictionary containing real-time data for the specified stock symbol, structured as follows:
    {
        "instrument_token": int,  # Unique identifier for the stock instrument
        "timestamp": str,         # Timestamp of the last update
        "last_trade_time": str,   # Time of the last trade
        "last_price": float,      # Last traded price
        "last_quantity": int,     # Quantity of the last trade
        "buy_quantity": int,      # Total quantity available for buying
        "sell_quantity": int,     # Total quantity available for selling
        "volume": int,            # Total volume traded
        "average_price": float,   # Average price of trades
        "oi": int,                # Open interest
        "oi_day_high": float,     # Highest open interest for the day
        "oi_day_low": float,      # Lowest open interest for the day
        "net_change": float,      # Net change in price
        "lower_circuit_limit": float,  # Lower circuit limit price
        "upper_circuit_limit": float,  # Upper circuit limit price
        "ohlc": {                 # Open, High, Low, Close data
            "open": float,        # Opening price
            "high": float,        # Highest price
            "low": float,         # Lowest price
            "close": float        # Closing price
        },
        "depth": {                # Order book depth
            "buy": [              # List of buy orders
                {
                    "price": float,  # Price of the buy order
                    "quantity": int, # Quantity of the buy order
                    "orders": int    # Number of orders at this price
                },
                ...
            ],
            "sell": [             # List of sell orders
                {
                    "price": float,  # Price of the sell order
                    "quantity": int, # Quantity of the sell order
                    "orders": int    # Number of orders at this price
                },
                ...
            ]
        }
    }

    Raises:
    - HTTPException: If no instrument token is found for the stock symbol (404 Not Found).
    - HTTPException: If no real-time data is found for the stock symbol (404 Not Found).
    - HTTPException: If an error occurs during the fetching of real-time data (500 Internal Server Error).
    """
    try:
        instrument_token = zerodha_service.get_instrument_token("BSE", symbol)
        if not instrument_token:
            raise HTTPException(status_code=404, detail=f"No instrument token found for stock symbol {symbol}")
        realtime_data = zerodha_service.get_quote([instrument_token])
        if realtime_data is None or str(instrument_token) not in realtime_data:
            raise HTTPException(status_code=404, detail=f"No real-time data found for stock symbol {symbol}")
        return realtime_data[str(instrument_token)]
    except Exception as e:
        logger.error(f"Error fetching real-time data for {symbol}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


class PredictionRequest(BaseModel):
    data: List[float]


class BacktestRequest(BaseModel):
    stock_code: str
    days: str = '1year'
    model: str = 'lstm'

@app.post("/api/backtest")
async def backtest_stock(request: BacktestRequest):
    """
    Backtest the specified model (LSTM or ARIMA) for a given stock over a specified time period.

    Parameters:
    - stock_code (str): The stock symbol for which backtesting is requested (e.g., "RELIANCE").
    - days (str): The time frame for historical data. Default is '1year'.
                  It can be one of the following values: '1month', '3months', '1year', '5years'
    - model (str): The model to use for backtesting. Default is 'lstm'.
                   It can be either 'lstm' or 'arima'.

    Returns:
    - dict: A dictionary containing the backtesting results.

    Raises:
    - HTTPException: If no historical data is found or if an error occurs during backtesting.
    """
    try:
        logger.info(f"Received backtesting request for {request.stock_code} using {request.model.upper()} model")
        historical_data = await fetch_historical_data(request.stock_code, request.days)

        if historical_data is None or historical_data.empty:
            raise HTTPException(status_code=404, detail=f"No data found for stock symbol {request.stock_code}")

        if request.model.lower() == 'lstm':
            backtesting_results = backtest_lstm_model(request.stock_code, historical_data)
        elif request.model.lower() == 'arima':
            backtesting_results = backtest_arima_model(request.stock_code, historical_data)
        else:
            raise HTTPException(status_code=400, detail=f"Invalid model specified: {request.model}")

        logger.info(f"Successfully completed {request.model.upper()} backtesting for {request.stock_code}")

        return backtesting_results
    except HTTPException as he:
        raise he
    except Exception as e:
        logger.error(f"Error during {request.model.upper()} backtesting for {request.stock_code}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error during backtesting: {str(e)}")

@app.get("/api/backtest/{stock_code}")
async def backtest_stock_get(stock_code: str, days: str = '1year', model: str = 'lstm'):
    """
    GET endpoint for backtesting. This is a wrapper around the POST endpoint for easier testing.
    """
    request = BacktestRequest(stock_code=stock_code, days=days, model=model)
    return await backtest_stock(request)


@app.get("/api/test/news/{symbol}")
async def test_fetch_news(symbol: str):
    """
    Fetch news articles related to a specific stock symbol.

    This endpoint retrieves the latest news articles for the given stock symbol, providing
    relevant information that may impact stock performance and investor sentiment.

    Parameters:
    - symbol (str): The stock symbol for which to fetch news articles (e.g., "RELIANCE").

    Returns:
    - dict: A dictionary containing the news articles for the specified stock symbol.

    Raises:
    - HTTPException: If an error occurs during the fetching of news articles (500 Internal Server Error).
    """
    try:
        news_data = await fetch_news_data(symbol)
        return {"news": news_data}
    except Exception as e:
        logger.error(f"Error fetching news for {symbol}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
async def get_technical_indicators(symbol: str, timeFrame: str = '1year'):
    """
    Retrieve technical indicators for a specified stock symbol over a given time frame.

    This endpoint calculates various technical indicators based on historical stock data, including:
    - Simple Moving Average (SMA)
    - Exponential Moving Average (EMA)
    - Relative Strength Index (RSI)
    - Moving Average Convergence Divergence (MACD)
    - Bollinger Bands
    - Average True Range (ATR)

    Parameters:
    - symbol (str): The stock symbol for which to retrieve technical indicators (e.g., "RELIANCE").
    - timeFrame (str): The time frame for historical data (default is '1year').

    Returns:
    - dict: A dictionary containing the calculated technical indicators and historical data, structured as follows:
        {
            "sma_20": List[float],  # 20-period SMA values
            "ema_50": List[float],  # 50-period EMA values
            "rsi_14": List[float],   # 14-period RSI values
            "macd": List[float],     # MACD values
            "macd_signal": List[float],  # MACD signal line values
            "bollinger_upper": List[float],  # Upper Bollinger Band values
            "bollinger_middle": List[float],  # Middle Bollinger Band values
            "bollinger_lower": List[float],  # Lower Bollinger Band values
            "atr": List[float],      # Average True Range values
            "dates": List[str],      # Dates corresponding to the historical data
            "close_prices": List[float]  # Closing prices for the historical data
        }

    Raises:
    - HTTPException: If no historical data is found for the specified stock symbol (404 Not Found).
    - HTTPException: If an error occurs during the calculation of technical indicators (500 Internal Server Error).
    """
    try:
        historical_data = await fetch_historical_data(symbol, timeFrame)
        if historical_data is None:
            raise HTTPException(status_code=404, detail=f"No data found for stock symbol {symbol}")

        logger.info(f"Columns in the dataframe: {historical_data.columns}")
        logger.info(f"Number of data points: {len(historical_data)}")

        # Calculate technical indicators
        sma_20 = calculate_sma(historical_data['close'], 20)
        ema_50 = calculate_ema(historical_data['close'], 50)
        rsi_14 = calculate_rsi(historical_data['close'], 14)
        macd, signal, _ = calculate_macd(historical_data['close'])
        upper, middle, lower = calculate_bollinger_bands(historical_data['close'])
        atr = calculate_atr(historical_data['high'], historical_data['low'], historical_data['close'], 14)

        def safe_float_list(arr):
            return [float(x) if not np.isnan(x) and not np.isinf(x) else None for x in arr]

        return {
            "sma_20": safe_float_list(sma_20),
            "ema_50": safe_float_list(ema_50),
            "rsi_14": safe_float_list(rsi_14),
            "macd": safe_float_list(macd),
            "macd_signal": safe_float_list(signal),
            "bollinger_upper": safe_float_list(upper),
            "bollinger_middle": safe_float_list(middle),
            "bollinger_lower": safe_float_list(lower),
            "atr": safe_float_list(atr),
            "dates": historical_data['date'].tolist(),
            "close_prices": safe_float_list(historical_data['close'])
        }
    except Exception as e:
        logger.error(f"Error calculating technical indicators for {symbol}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/stocks/{symbol}/realtime")
async def get_realtime_data(symbol: str):
    """
    Fetch real-time market data for a specified stock symbol.

    This endpoint retrieves the latest trading information for the given stock symbol, including
    instrument token, last price, volume, and order book depth.

    Parameters:
    - symbol (str): The stock symbol for which to fetch real-time data (e.g., "RELIANCE").

    Returns:
    - dict: A dictionary containing real-time data for the specified stock symbol, structured as follows:
    {
        "instrument_token": int,  # Unique identifier for the stock instrument
        "timestamp": str,         # Timestamp of the last update
        "last_trade_time": str,   # Time of the last trade
        "last_price": float,      # Last traded price
        "last_quantity": int,     # Quantity of the last trade
        "buy_quantity": int,      # Total quantity available for buying
        "sell_quantity": int,     # Total quantity available for selling
        "volume": int,            # Total volume traded
        "average_price": float,   # Average price of trades
        "oi": int,                # Open interest
        "oi_day_high": float,     # Highest open interest for the day
        "oi_day_low": float,      # Lowest open interest for the day
        "net_change": float,      # Net change in price
        "lower_circuit_limit": float,  # Lower circuit limit price
        "upper_circuit_limit": float,  # Upper circuit limit price
        "ohlc": {                 # Open, High, Low, Close data
            "open": float,        # Opening price
            "high": float,        # Highest price
            "low": float,         # Lowest price
            "close": float        # Closing price
        },
        "depth": {                # Order book depth
            "buy": [              # List of buy orders
                {
                    "price": float,  # Price of the buy order
                    "quantity": int, # Quantity of the buy order
                    "orders": int    # Number of orders at this price
                },
                ...
            ],
            "sell": [             # List of sell orders
                {
                    "price": float,  # Price of the sell order
                    "quantity": int, # Quantity of the sell order
                    "orders": int    # Number of orders at this price
                },
                ...
            ]
        }
    }

    Raises:
    - HTTPException: If no instrument token is found for the stock symbol (404 Not Found).
    - HTTPException: If no real-time data is found for the stock symbol (404 Not Found).
    - HTTPException: If an error occurs during the fetching of real-time data (500 Internal Server Error).
    """
    try:
        instrument_token = zerodha_service.get_instrument_token("BSE", symbol)
        if not instrument_token:
            raise HTTPException(status_code=404, detail=f"No instrument token found for stock symbol {symbol}")
        realtime_data = zerodha_service.get_quote([instrument_token])
        if realtime_data is None or str(instrument_token) not in realtime_data:
            raise HTTPException(status_code=404, detail=f"No real-time data found for stock symbol {symbol}")
        return realtime_data[str(instrument_token)]
    except Exception as e:
        logger.error(f"Error fetching real-time data for {symbol}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/predict/{stock_code}")
async def predict_stock(stock_code: str):
    try:
        logger.info(f"Received prediction request for {stock_code}")
        model_dir = os.path.join(os.path.dirname(__file__), 'models')
        lstm_model_path = os.path.join(model_dir, f'{stock_code}_lstm_model.h5')
        scaler_path = os.path.join(model_dir, f'{stock_code}_scaler.pkl')
        arima_model_path = os.path.join(model_dir, f'{stock_code}_arima_model.pkl')

        if not os.path.exists(lstm_model_path) or not os.path.exists(scaler_path) or not os.path.exists(arima_model_path):
            logger.error(f"Models or scaler not found for {stock_code}")
            raise HTTPException(status_code=404, detail=f"Models not found for {stock_code}")

        lstm_predictor = LSTMStockPredictor(input_shape=(60, 1))
        lstm_predictor.load_model(lstm_model_path)
        scaler = joblib.load(scaler_path)
        arima_predictor = joblib.load(arima_model_path)

        # Fetch historical data
        historical_data = await fetch_historical_data(stock_code, '1year')
        if historical_data is None:
            raise HTTPException(status_code=404, detail=f"No historical data found for {stock_code}")

        close_prices = historical_data['close'].values.reshape(-1, 1)
        scaled_data = scaler.transform(close_prices)

        # Generate past predictions for LSTM
        lstm_past_predictions = []
        for i in range(60, len(scaled_data)):
            X = scaled_data[i-60:i].reshape(1, 60, 1)
            prediction = lstm_predictor.predict(X)
            lstm_past_predictions.append(prediction[0][0])

        lstm_past_predictions = scaler.inverse_transform(np.array(lstm_past_predictions).reshape(-1, 1))

        # Generate future predictions for LSTM
        lstm_future_predictions = []
        last_sequence = scaled_data[-60:].reshape(1, 60, 1)
        for _ in range(7):  # Predict next 7 days
            prediction = lstm_predictor.predict(last_sequence)
            lstm_future_predictions.append(prediction[0][0])
            last_sequence = np.roll(last_sequence, -1, axis=1)
            last_sequence[0, -1, 0] = prediction[0][0]

        lstm_future_predictions = scaler.inverse_transform(np.array(lstm_future_predictions).reshape(-1, 1))

        # Generate predictions for ARIMA
        arima_past_predictions = arima_predictor.predict(len(close_prices) - 60)
        arima_future_predictions = arima_predictor.predict(7)

        logger.info(f"Successfully generated predictions for {stock_code}")

        return {
            "lstm_past_predictions": lstm_past_predictions.flatten().tolist(),
            "lstm_future_predictions": lstm_future_predictions.flatten().tolist(),
            "arima_past_predictions": arima_past_predictions.tolist(),
            "arima_future_predictions": arima_future_predictions.tolist()
        }
    except Exception as e:
        logger.error(f"Error making prediction for {stock_code}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error making prediction for {stock_code}: {str(e)}")


@app.get("/api/sentiment/{symbol}")
async def get_sentiment(symbol: str):
    try:
        logger.info(f"Fetching news data for {symbol}")
        news_data = await fetch_news_data(symbol)
        logger.info(f"Fetched {len(news_data)} news articles for {symbol}")

        historical_data = await fetch_historical_data(symbol, '1year')
        if historical_data is None:
            raise HTTPException(status_code=404, detail=f"No data found for stock symbol {symbol}")

        close_prices = historical_data['close'].tolist()
        prediction_request = PredictionRequest(data=close_prices)
        lstm_prediction = await predict_stock(symbol, prediction_request)

        logger.info(f"Processing news data for {symbol}")
        news_analysis = await llm_processor.process_news_data(news_data, lstm_prediction['predictions'][0], symbol, historical_data)
        logger.info(f"Sentiment analysis result for {symbol}: {news_analysis}")

        return news_analysis
    except Exception as e:
        logger.error(f"Error analyzing sentiment for {symbol}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/test/news/{symbol}")
async def test_fetch_news(symbol: str):
    """
    Fetch news articles related to a specific stock symbol.

    This endpoint retrieves the latest news articles for the given stock symbol, providing
    relevant information that may impact stock performance and investor sentiment.

    Parameters:
    - symbol (str): The stock symbol for which to fetch news articles (e.g., "RELIANCE").

    Returns:
    - dict: A dictionary containing a list of news articles structured as follows:
    {
        "news": [
            {
                "title": str,            # Title of the news article
                "link": str,             # URL link to the full article
                "published_time": str,   # Publication time of the article in RFC 2822 format
                "summary": str           # Summary or excerpt of the article
            },
            ...
        ]
    }

    Raises:
    - HTTPException: If an error occurs during the fetching of news articles (500 Internal Server Error).
    """
    try:
        news_data = await fetch_news_data(symbol)
        return {"news": news_data}
    except Exception as e:
        logger.error(f"Error fetching news for {symbol}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


async def get_stock_analysis(stock_code: str):
    try:
        # Fetch necessary data
        news_data = await fetch_news_data(stock_code)
        historical_data = await fetch_historical_data(stock_code)

        if historical_data is None:
            raise HTTPException(status_code=404, detail=f"No historical data found for stock code {stock_code}")

        technical_indicators = {
            "sma_20": calculate_sma(historical_data['close'], 20),
            "ema_50": calculate_ema(historical_data['close'], 50),
            "rsi_14": calculate_rsi(historical_data['close'], 14),
            "macd": calculate_macd(historical_data['close']),
            "bollinger_bands": calculate_bollinger_bands(historical_data['close']),
            "atr": calculate_atr(historical_data['high'], historical_data['low'], historical_data['close'], 14)
        }

        lstm_prediction = await get_lstm_prediction(stock_code)

        gpt4_processor = GPT4Processor()
        news_sentiment, sentiment_explanation = gpt4_processor.analyze_news_sentiment(news_data)

        final_analysis = await gpt4_processor.final_analysis(
            stock_code,
            news_sentiment,
            sentiment_explanation,
            lstm_prediction,
            technical_indicators,
            historical_data
        )

        return {
            "stock_code": stock_code,
            "news_sentiment": news_sentiment,
            "sentiment_explanation": sentiment_explanation,
            "lstm_prediction": lstm_prediction,
            "technical_indicators": technical_indicators,
            "final_analysis": final_analysis
        }
    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"Error generating analysis for {stock_code}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error generating analysis for {stock_code}: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

================
File: app/models/backtesting.py
================
import numpy as np
import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_error, mean_squared_error
from tensorflow.keras.models import load_model
import joblib
import logging
import os
from pydantic import BaseModel
from app.models.backtesting import backtest_lstm_model, backtest_arima_model
from app.services.data_collection import fetch_historical_data

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ARIMAStockPredictor:
    def __init__(self, order=(1, 1, 1)):
        self.order = order
        self.model = None

    def train(self, data):
        self.model = ARIMA(data, order=self.order)
        self.model_fit = self.model.fit()

    def predict(self, steps):
        forecast = self.model_fit.forecast(steps)
        return np.array(forecast)

    def evaluate(self, y_test, predictions):
        mae = mean_absolute_error(y_test, predictions)
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        return mae, rmse


class LSTMStockPredictor:
    def __init__(self, input_shape=(60, 1)):
        self.model = None
        self.input_shape = input_shape

    def load_model(self, model_path):
        self.model = load_model(model_path)

    def predict(self, X):
        return self.model.predict(X)

    def evaluate(self, X_test, y_test):
        predictions = self.predict(X_test)
        mae = mean_absolute_error(y_test, predictions)
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        return mae, rmse


def backtest_model(stock_code, historical_data, model_type, investment_amount=10000, threshold=0.01, transaction_cost=0.001, stop_loss_percentage=0.05):
    try:
        close_prices = historical_data['close'].values
        cash = investment_amount
        shares = 0
        trades = []
        portfolio_values = [investment_amount]
        buy_price = 0
        mae = 0
        rmse = 0
        final_portfolio_value = investment_amount

        if model_type == 'LSTM':
            model_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'models')
            model_path = os.path.join(model_dir, f'{stock_code}_lstm_model.h5')
            scaler_path = os.path.join(model_dir, f'{stock_code}_scaler.pkl')

            if not os.path.exists(model_path) or not os.path.exists(scaler_path):
                raise FileNotFoundError(f"Model or scaler not found for {stock_code}")

            predictor = LSTMStockPredictor()
            predictor.load_model(model_path)
            scaler = joblib.load(scaler_path)

            scaled_data = scaler.transform(close_prices.reshape(-1, 1))
            X, y = [], []
            for i in range(60, len(scaled_data)):
                X.append(scaled_data[i-60:i, 0])
                y.append(scaled_data[i, 0])
            X, y = np.array(X), np.array(y)

            predictions = predictor.predict(X)
            predictions = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()
            actual_prices = scaler.inverse_transform(y.reshape(-1, 1)).flatten()

            logger.info(f"LSTM Predictions for {stock_code}:")
            for i in range(len(predictions)):
                current_price = actual_prices[i]
                current_prediction = predictions[i]

                if i > 0:
                    prev_price = actual_prices[i-1]

                    if shares > 0 and current_price < buy_price * (1 - stop_loss_percentage):
                        cash += shares * current_price * (1 - transaction_cost)
                        trades.append(('sell', shares, current_price, 'stop-loss'))
                        shares = 0
                    elif current_prediction > prev_price * (1 + threshold) and cash > current_price:
                        shares_to_buy = (cash * (1 - transaction_cost)) // current_price
                        if shares_to_buy > 0:
                            shares += shares_to_buy
                            cash -= shares_to_buy * current_price * (1 + transaction_cost)
                            trades.append(('buy', shares_to_buy, current_price))
                            buy_price = current_price
                    elif current_prediction < prev_price * (1 - threshold) and shares > 0:
                        cash += shares * current_price * (1 - transaction_cost)
                        trades.append(('sell', shares, current_price))
                        shares = 0

                portfolio_value = cash + shares * current_price
                portfolio_values.append(portfolio_value)

            final_portfolio_value = cash + shares * actual_prices[-1]

        elif model_type == 'ARIMA':
            # ARIMA model implementation
            model = ARIMA(close_prices, order=(1, 1, 1))
            model_fit = model.fit()
            predictions = model_fit.forecast(steps=len(close_prices))
            actual_prices = close_prices

            mae = np.mean(np.abs(predictions - actual_prices))
            rmse = np.sqrt(np.mean((predictions - actual_prices)**2))

            for i in range(1, len(predictions)):
                current_price = actual_prices[i]
                prev_price = actual_prices[i-1]
                prev_prediction = predictions[i-1]

                if shares > 0 and current_price < buy_price * (1 - stop_loss_percentage):
                    cash += shares * current_price * (1 - transaction_cost)
                    trades.append(('sell', shares, current_price, 'stop-loss'))
                    shares = 0
                elif prev_prediction > prev_price * (1 + threshold) and cash > current_price:
                    shares_to_buy = (cash * (1 - transaction_cost)) // current_price
                    if shares_to_buy > 0:
                        shares += shares_to_buy
                        cash -= shares_to_buy * current_price * (1 + transaction_cost)
                        trades.append(('buy', shares_to_buy, current_price))
                        buy_price = current_price
                elif prev_prediction < prev_price * (1 - threshold) and shares > 0:
                    cash += shares * current_price * (1 - transaction_cost)
                    trades.append(('sell', shares, current_price))
                    shares = 0

                portfolio_value = cash + shares * current_price
                portfolio_values.append(portfolio_value)

            final_portfolio_value = cash + shares * actual_prices[-1]

        else:
            raise ValueError(f"Invalid model type: {model_type}")

        total_return = (final_portfolio_value - investment_amount) / investment_amount * 100

        return {
            'stock_code': stock_code,
            'model_type': model_type,
            'initial_investment': investment_amount,
            'final_portfolio_value': final_portfolio_value,
            'total_return_percentage': total_return,
            'number_of_trades': len(trades),
            'trades': trades,
            'portfolio_values': portfolio_values,
            'mae': mae,
            'rmse': rmse,
            'predictions': predictions.tolist(),
            'actual_values': actual_prices.tolist()
        }

    except Exception as e:
        logger.error(f"Error in backtesting {model_type} model for {stock_code}: {str(e)}")
        raise


def backtest_lstm_model(stock_code, historical_data, investment_amount=10000, threshold=0.01, transaction_cost=0.001):
    return backtest_model(stock_code, historical_data, 'LSTM', investment_amount, threshold, transaction_cost)


def backtest_arima_model(stock_code, historical_data, investment_amount=10000, threshold=0.01, transaction_cost=0.001):
    return backtest_model(stock_code, historical_data, 'ARIMA', investment_amount, threshold, transaction_cost)

================
File: app/models/arima_model.py
================
import numpy as np
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_error, mean_squared_error


class ARIMAStockPredictor:
    def __init__(self, order=(1, 1, 1)):
        self.order = order
        self.model = None

    def train(self, data):
        self.model = ARIMA(data, order=self.order)
        self.model_fit = self.model.fit()

    def predict(self, steps):
        return self.model_fit.forecast(steps)

    def evaluate(self, y_test, predictions):
        mae = mean_absolute_error(y_test, predictions)
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        return mae, rmse

================
File: app/models/train_and_save_model.py
================
import pandas as pd
import numpy as np
from lstm_model import LSTMStockPredictor
from arima_model import ARIMAStockPredictor
import sys
import os
import json
import csv
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from app.services.data_collection import fetch_historical_data
import asyncio
import logging
from sklearn.model_selection import train_test_split
import joblib
from concurrent.futures import ProcessPoolExecutor, as_completed
from datetime import datetime, timedelta
import traceback
import time
from tenacity import retry, stop_after_attempt, wait_exponential

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

PROGRESS_FILE = os.path.join(os.path.dirname(__file__), 'training_progress.json')
FAILED_STOCKS_FILE = os.path.join(os.path.dirname(__file__), 'failed_stocks.csv')

MAX_CONSECUTIVE_ERRORS = 10
PAUSE_DURATION = 60  # 1 minute

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
async def get_data_for_stock(stock_code, time_frame='5years'):
    time.sleep(0.7) # 0.7 seconds delay to avoid rate limiting
    try:
        data = await fetch_historical_data(stock_code, time_frame)
        if data is not None and not data.empty:
            return data
        else:
            logger.warning(f"No data fetched for {stock_code}")
            save_failed_stock(stock_code, "No data fetched")
    except Exception as e:
        logger.error(f"Error fetching data for {stock_code}: {str(e)}")
        save_failed_stock(stock_code, f"Error fetching data: {str(e)}")
        raise  # Re-raise the exception to trigger a retry
    return None


def train_and_save_model_for_stock(stock_code, data):
    try:
        logger.info(f"Training models for {stock_code}")
        if data.empty:
            logger.warning(f"No data available for {stock_code}")
            return stock_code, None, None

        close_prices = data['close'].values.reshape(-1, 1)

        # Train LSTM model
        lstm_predictor = LSTMStockPredictor(input_shape=(60, 1))
        X, y, scaler = lstm_predictor.preprocess_data(close_prices)
        if len(X) < 100:  # Arbitrary minimum length for training
            logger.warning(f"Insufficient data for {stock_code} to train LSTM model")
            return stock_code, None, None
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        history = lstm_predictor.train(X_train, y_train, epochs=100, batch_size=32, validation_split=0.1)
        lstm_mae, lstm_rmse = lstm_predictor.evaluate(X_test, y_test)
        logger.info(f"LSTM Model Evaluation for {stock_code} - MAE: {lstm_mae}, RMSE: {lstm_rmse}")

        # Train ARIMA model
        arima_predictor = ARIMAStockPredictor()
        arima_predictor.train(close_prices.flatten())
        arima_mae, arima_rmse = arima_predictor.evaluate(close_prices[-len(X_test):].flatten())
        logger.info(f"ARIMA Model Evaluation for {stock_code} - MAE: {arima_mae}, RMSE: {arima_rmse}")

        model_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'models')
        os.makedirs(model_dir, exist_ok=True)
        
        lstm_model_path = os.path.join(model_dir, f'{stock_code}_lstm_model.h5')
        arima_model_path = os.path.join(model_dir, f'{stock_code}_arima_model.pkl')
        scaler_path = os.path.join(model_dir, f'{stock_code}_scaler.pkl')
        
        lstm_predictor.save_model(lstm_model_path)
        joblib.dump(arima_predictor, arima_model_path)
        joblib.dump(scaler, scaler_path)

        logger.info(f"Models and scaler for {stock_code} saved successfully.")
        return stock_code, (lstm_mae, lstm_rmse), (arima_mae, arima_rmse)
    except Exception as e:
        logger.error(f"Error training models for {stock_code}: {str(e)}")
        logger.error(traceback.format_exc())
        return stock_code, None, None


def load_progress():
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, 'r') as f:
            return json.load(f)
    return {'completed': [], 'last_index': 0, 'errors': []}


def save_progress(completed, last_index, errors):
    with open(PROGRESS_FILE, 'w') as f:
        json.dump({'completed': list(completed), 'last_index': last_index, 'errors': errors}, f)


def save_failed_stock(stock_code, reason):
    with open(FAILED_STOCKS_FILE, 'a', newline='') as f:
        writer = csv.writer(f)
        writer.writerow([stock_code, reason, datetime.now().strftime("%Y-%m-%d %H:%M:%S")])


async def train_models(stock_codes):
    progress = load_progress()
    completed = set(progress['completed'])
    start_index = progress['last_index']
    errors = progress['errors']
    
    results = []
    total_stocks = len(stock_codes)
    with ProcessPoolExecutor() as executor:
        futures = []
        for i, stock_code in enumerate(stock_codes[start_index:], start=start_index):
            if stock_code in completed:
                continue
            data = await get_data_for_stock(stock_code)
            if data is not None:
                future = executor.submit(train_and_save_model_for_stock, stock_code, data)
                futures.append(future)
            else:
                logger.error(f"Skipping model training for {stock_code} due to missing data.")
                errors.append(f"Missing data for {stock_code}")
                save_failed_stock(stock_code, "Missing data")
            
            # Save progress after each stock is processed (whether successful or not)
            completed.add(stock_code)
            save_progress(completed, i + 1, errors)
            logger.info(f"Progress: {len(completed)}/{total_stocks} stocks processed")
        
        for future in as_completed(futures):
            try:
                result = future.result()
                results.append(result)
                logger.info(f"Completed training for {result[0]}")
                # Update progress after each model is trained
                completed.add(result[0])
                save_progress(completed, start_index + len(completed), errors)
            except Exception as e:
                logger.error(f"Error processing future: {str(e)}")
                logger.error(traceback.format_exc())
                errors.append(f"Error processing {result[0] if result else 'unknown stock'}: {str(e)}")
            
            # Update progress after each future is completed
            save_progress(completed, start_index + len(completed), errors)
    
    return results


def should_retrain(model_path, retrain_interval_days=30):
    if not os.path.exists(model_path):
        return True
    last_modified = datetime.fromtimestamp(os.path.getmtime(model_path))
    return (datetime.now() - last_modified) > timedelta(days=retrain_interval_days)


def delete_existing_models(model_dir):
    for file in os.listdir(model_dir):
        if file.endswith(('.h5', '.pkl')):
            os.remove(os.path.join(model_dir, file))
    logger.info("Deleted existing models")


async def main():
    try:
        csv_path = os.path.join(os.path.dirname(__file__), '..', '..', 'scripts', 'indian_stocks.csv')
        stocks_df = pd.read_csv(csv_path)
        stock_codes = stocks_df['tradingsymbol'].tolist()

        model_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'models')
        
        # Uncomment the following line if you want to delete existing models before training
        # delete_existing_models(model_dir)

        progress = load_progress()
        start_index = progress['last_index']
        completed = set(progress['completed'])

        stocks_to_train = [
            stock_code for stock_code in stock_codes[start_index:]
            if stock_code not in completed and should_retrain(os.path.join(model_dir, f'{stock_code}_lstm_model.h5'))
        ]

        if stocks_to_train:
            results = await train_models(stocks_to_train)
            for stock_code, lstm_metrics, arima_metrics in results:
                if lstm_metrics and arima_metrics:
                    logger.info(f"Training completed for {stock_code} - LSTM: MAE: {lstm_metrics[0]}, RMSE: {lstm_metrics[1]} - ARIMA: MAE: {arima_metrics[0]}, RMSE: {arima_metrics[1]}")
                else:
                    logger.warning(f"Training failed for {stock_code}")
        else:
            logger.info("No models need retraining at this time.")

        # Clean up progress file after successful completion
        if os.path.exists(PROGRESS_FILE):
            os.remove(PROGRESS_FILE)
            logger.info("Training process completed successfully. Progress file removed.")
    except Exception as e:
        logger.error(f"An error occurred in the main function: {str(e)}")
        logger.error(traceback.format_exc())


if __name__ == "__main__":
    asyncio.run(main())

================
File: app/models/lstm_model.py
================
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt


class LSTMStockPredictor:
    def __init__(self, input_shape, lstm_units=50, dropout_rate=0.2):
        self.model = Sequential()
        self.model.add(LSTM(units=lstm_units, return_sequences=True, input_shape=input_shape))
        self.model.add(Dropout(dropout_rate))
        self.model.add(LSTM(units=lstm_units))
        self.model.add(Dropout(dropout_rate))
        self.model.add(Dense(1))
        self.model.compile(optimizer='adam', loss='mean_squared_error')

    def preprocess_data(self, data, sequence_length=60):
        scaler = MinMaxScaler(feature_range=(0, 1))
        scaled_data = scaler.fit_transform(data)
        X, y = [], []
        for i in range(sequence_length, len(scaled_data)):
            X.append(scaled_data[i-sequence_length:i, 0])
            y.append(scaled_data[i, 0])
        X, y = np.array(X), np.array(y)
        X = np.reshape(X, (X.shape[0], X.shape[1], 1))
        return X, y, scaler

    def train(self, X_train, y_train, epochs=100, batch_size=32, validation_split=0.1):
        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
        history = self.model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=validation_split,
            callbacks=[early_stopping]
        )
        return history

    def predict(self, X):
        return self.model.predict(X)

    def evaluate(self, X_test, y_test):
        predictions = self.predict(X_test)
        mae = mean_absolute_error(y_test, predictions)
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        return mae, rmse

    def plot_predictions(self, y_test, predictions):
        plt.figure(figsize=(14, 5))
        plt.plot(y_test, color='blue', label='Actual Stock Price')
        plt.plot(predictions, color='red', label='Predicted Stock Price')
        plt.title('Stock Price Prediction')
        plt.xlabel('Time')
        plt.ylabel('Stock Price')
        plt.legend()
        plt.show()

    def save_model(self, file_path):
        self.model.save(file_path)

    def load_model(self, file_path):
        self.model = tf.keras.models.load_model(file_path)


# Example usage:
# data = pd.read_csv('/Users/priyanshsingh/Developer/Projects/Advanced-Stock-Analysis/backend/data/500027.csv')
# predictor = LSTMStockPredictor(input_shape=(60, 1))
# X, y, scaler = predictor.preprocess_data(data['Close Price'].values.reshape(-1, 1))
# history = predictor.train(X, y)
# predictions = predictor.predict(X)
# mae, rmse = predictor.evaluate(X, y)
# predictor.plot_predictions(y, predictions)

================
File: app/db/influx_writer.py
================
import os
import pandas as pd
import logging
from influxdb_client import Point, WritePrecision
from influxdb_client.client.write_api import SYNCHRONOUS  # Ensure this is imported
from ..db.influx_client import get_influxdb_client
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor

# Script Purpose:
# This script processes stock CSV files, calculates technical indicators (including RSI),
# and ingests the data into an InfluxDB instance. It handles multiple CSV files, processes
# them in batches, and optionally runs the processing in parallel for faster execution.

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

bucket = "stock_data"
client = get_influxdb_client()


def calculate_rsi(data, window=14):
    """Calculates the Relative Strength Index (RSI) with a default 14-day window."""
    delta = data.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi


def read_and_process_csv(file_path):
    try:
        # Log start of file processing
        logger.info(f"Starting processing for {file_path}")

        # Read the CSV
        df = pd.read_csv(file_path)

        # Rename columns to match expected names
        df.rename(columns={
            'Open Price': 'Open',
            'High Price': 'High',
            'Low Price': 'Low',
            'Close Price': 'Close',
            'No.of Shares': 'Volume'
        }, inplace=True)

        # Log the columns found in the CSV after renaming
        logger.info(f"Columns found in {file_path}: {list(df.columns)}")

        # Validate essential columns
        required_columns = ['Date', 'Close', 'Open', 'High', 'Low', 'Volume']
        if not all(column in df.columns for column in required_columns):
            raise ValueError(f"Missing essential columns in {file_path}")

        # Convert columns to numeric values (handling commas)
        df['Close'] = pd.to_numeric(df['Close'], errors='coerce')
        df['Open'] = pd.to_numeric(df['Open'], errors='coerce')
        df['High'] = pd.to_numeric(df['High'], errors='coerce')
        df['Low'] = pd.to_numeric(df['Low'], errors='coerce')
        df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce')

        # Convert 'Date' to datetime
        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')

        # Drop rows with missing or malformed data
        df.dropna(subset=['Date', 'Close', 'Open', 'High', 'Low', 'Volume'], inplace=True)

        # Calculate indicators
        df['RSI'] = calculate_rsi(df['Close'])
        df['Moving_Average_50'] = df['Close'].rolling(window=50).mean()
        df['Moving_Average_200'] = df['Close'].rolling(window=200).mean()
        df['Volatility'] = df['Close'].rolling(window=20).std()

        return df

    except FileNotFoundError as fnf_error:
        logger.error(f"File not found: {file_path} - {fnf_error}")
    except ValueError as val_error:
        logger.error(f"Value error in {file_path}: {val_error}")
    except Exception as e:
        logger.error(f"Error processing {file_path}: {str(e)}")
        return None


def write_stock_data_to_influxdb(df, symbol, batch_size=100):
    try:
        # Batch processing
        write_api = client.write_api(write_options=SYNCHRONOUS)
        points = []

        for i, row in df.iterrows():
            point = (
                Point("stock_data")
                .tag("symbol", symbol)
                .field("close", row['close'])
                .field("open", row['open'])
                .field("high", row['high'])
                .field("low", row['low'])
                .field("volume", row['volume'])
                .field("rsi", row['RSI'])
                .field("sma_20", row['SMA_20'])
                .field("ema_50", row['EMA_50'])
                .field("macd", row['MACD'])
                .field("macd_signal", row['MACD_Signal'])
                .field("macd_hist", row['MACD_Hist'])
                .field("bollinger_upper", row['Bollinger_Upper'])
                .field("bollinger_middle", row['Bollinger_Middle'])
                .field("bollinger_lower", row['Bollinger_Lower'])
                .field("atr", row['ATR'])
                .time(row['date'], WritePrecision.NS)
            )
            points.append(point)

            # Write in batches of 'batch_size'
            if len(points) >= batch_size:
                write_api.write(bucket=bucket, org="my_org", record=points)
                points.clear()  # Clear the list after writing the batch

        # Write any remaining points
        if points:
            write_api.write(bucket=bucket, org="my_org", record=points)

        logger.info(f"Data for {symbol} written to InfluxDB.")
    except Exception as e:
        logger.error(f"Error writing data for {symbol} to InfluxDB: {str(e)}")


def process_all_csv_files():
    data_folder = os.path.join(os.path.dirname(__file__), '../../data/')

    # Use a progress bar
    csv_files = [f for f in os.listdir(data_folder) if f.endswith('.csv')]
    with tqdm(total=len(csv_files)) as progress_bar:
        for file_name in csv_files:
            file_path = os.path.join(data_folder, file_name)
            symbol = file_name.split('.')[0]
            df = read_and_process_csv(file_path)
            if df is not None:
                write_stock_data_to_influxdb(df, symbol)
            progress_bar.update(1)


def process_files_in_parallel():
    data_folder = os.path.join(os.path.dirname(__file__), '../../data/')
    csv_files = [os.path.join(data_folder, f) for f in os.listdir(data_folder) if f.endswith('.csv')]

    # Limit the number of concurrent processes
    with ProcessPoolExecutor(max_workers=8) as executor:  # Adjust based on your CPU and system load
        list(tqdm(executor.map(process_csv_file, csv_files), total=len(csv_files)))


def process_csv_file(file_path):
    symbol = os.path.basename(file_path).split('.')[0]
    df = read_and_process_csv(file_path)
    if df is not None:
        write_stock_data_to_influxdb(df, symbol)


if __name__ == "__main__":
    # Uncomment either of these based on your preference
    # process_all_csv_files()  # Single-threaded
    process_files_in_parallel()  # Parallel processing

================
File: app/db/influx_client.py
================
from influxdb_client import InfluxDBClient
import os

def get_influxdb_client():
    token = os.environ.get("INFLUXDB_TOKEN")
    org = "my_org"
    url = "http://localhost:8086"

    # Set longer timeouts for handling larger loads in parallel processing
    client = InfluxDBClient(url=url, token=token, org=org, timeout=60000)  # Set a 60s timeout

    return client

================
File: app/api/stocks.py
================
from ..db.influx_client import get_influxdb_client
from fastapi import APIRouter, HTTPException
from ..services.zerodha_service import ZerodhaService
import logging

# ZerodhaService instance to fetch live stock data
zerodha_service = ZerodhaService()

router = APIRouter()

logger = logging.getLogger(__name__)


@router.get("/stocks/{symbol}/data")
async def get_stock_data(symbol: str):
    try:
        client = get_influxdb_client()
        query_api = client.query_api()

        query = f'''
        from(bucket: "stock_data")
          |> range(start: -730d)
          |> filter(fn: (r) => r._measurement == "stock_data" and r.symbol == "{symbol}")
          |> keep(columns: ["_time", "_value", "_field", "symbol"])
        '''

        logger.info(f"Executing InfluxDB query for symbol: {symbol}")
        result = query_api.query(org="my_org", query=query)
        logger.info(f"Query result: {result}")

        stock_data = []
        for table in result:
            for record in table.records:
                stock_data.append({
                    "time": record["_time"],
                    "field": record["_field"],
                    "value": record["_value"]
                })

        logger.info(f"Processed {len(stock_data)} data points for {symbol}")
        return {"symbol": symbol, "data": stock_data}

    except Exception as e:
        logger.error(f"Error fetching stock data for {symbol}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/stocks/{symbol}/live")
async def get_live_stock_data(symbol: str):
    try:
        # Fetch live data for the stock symbol using Zerodha's API
        live_data = zerodha_service.get_quote(f"BSE:{symbol}")

        # Check if data is returned
        if not live_data or f"BSE:{symbol}" not in live_data:
            raise HTTPException(status_code=404, detail=f"No live data found for stock symbol {symbol}")

        # Extract and return the relevant data
        return live_data[f"BSE:{symbol}"]

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fetching live data: {str(e)}")

================
File: app/services/data_processor.py
================
import pandas as pd
import numpy as np
from typing import Dict, Any, List
import math


def is_json_serializable(x):
    try:
        json.dumps(x)
        return True
    except (TypeError, OverflowError):
        return False

def sanitize_value(value):
    if isinstance(value, (int, float)):
        if not math.isfinite(value):
            return None
    return value if is_json_serializable(value) else str(value)


def calculate_rsi(data: pd.Series, window: int = 14) -> pd.Series:
    delta = data.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    return 100 - (100 / (1 + rs))


def process_bse_data(raw_data: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "symbol": raw_data.get("scripCode"),
        "name": raw_data.get("companyName"),
        "price": raw_data.get("currentValue"),
        "change": raw_data.get("change"),
        "percent_change": raw_data.get("pChange"),
        "volume": raw_data.get("totalTradedQuantity"),
        "value": raw_data.get("totalTradedValue"),
        "high": raw_data.get("dayHigh"),
        "low": raw_data.get("dayLow"),
        "updated_on": raw_data.get("updatedOn")
    }


def process_multiple_bse_stocks(raw_data: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:
    processed_data = []
    for scrip_code, data in raw_data.items():
        df = pd.DataFrame([data])

        # Convert relevant columns to numeric
        numeric_columns = ['currentValue', 'previousClose', 'dayHigh', 'dayLow', 'totalTradedQuantity']
        for col in numeric_columns:
            df[col] = pd.to_numeric(df[col].replace(',', ''), errors='coerce')

        # Calculate additional metrics
        df['daily_return'] = (df['currentValue'] - df['previousClose']) / df['previousClose']
        df['volatility'] = (df['dayHigh'] - df['dayLow']) / df['previousClose']
        df['rsi'] = calculate_rsi(df['currentValue'])

        processed_stock = df.to_dict(orient='records')[0]
        processed_stock['scrip_code'] = scrip_code

        # Sanitize values
        processed_stock = {k: sanitize_value(v) for k, v in processed_stock.items()}

        processed_data.append(processed_stock)

    return processed_data


def process_top_gainers_losers(raw_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    return [
        {
            "symbol": item.get("scripCode"),
            "name": item.get("securityID"),
            "price": item.get("LTP"),
            "change": item.get("change"),
            "percent_change": item.get("pChange")
        }
        for item in raw_data
    ]

def process_indices(raw_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    indices = raw_data.get("indices", [])
    return [
        {
            "name": index.get("name"),
            "value": index.get("currentValue"),
            "change": index.get("change"),
            "percent_change": index.get("pChange")
        }
        for index in indices
    ]

================
File: app/services/technical_indicators.py
================
import pandas as pd
import numpy as np

def calculate_sma(data: pd.Series, window: int) -> pd.Series:
    return data.rolling(window=min(window, len(data)), min_periods=1).mean()

def calculate_ema(data: pd.Series, window: int) -> pd.Series:
    return data.ewm(span=window, adjust=False, min_periods=1).mean()

def calculate_rsi(data: pd.Series, window: int = 14) -> pd.Series:
    delta = data.diff()
    gain, loss = delta.copy(), delta.copy()
    gain[gain < 0] = 0
    loss[loss > 0] = 0
    avg_gain = gain.rolling(window=window, min_periods=1).mean()
    avg_loss = -loss.rolling(window=window, min_periods=1).mean()
    rs = avg_gain / avg_loss
    rs = rs.replace([np.inf, -np.inf], np.nan)
    return 100 - (100 / (1 + rs))

def calculate_macd(data: pd.Series, fast_period: int = 12, slow_period: int = 26, signal_period: int = 9) -> tuple:
    ema_fast = calculate_ema(data, fast_period)
    ema_slow = calculate_ema(data, slow_period)
    macd_line = ema_fast - ema_slow
    signal_line = calculate_ema(macd_line, signal_period)
    histogram = macd_line - signal_line
    return macd_line, signal_line, histogram

def calculate_bollinger_bands(data: pd.Series, window: int = 20, num_std: float = 2) -> tuple:
    sma = calculate_sma(data, window)
    std = data.rolling(window=min(window, len(data)), min_periods=1).std()
    upper_band = sma + (std * num_std)
    lower_band = sma - (std * num_std)
    return upper_band, sma, lower_band

def calculate_atr(high: pd.Series, low: pd.Series, close: pd.Series, window: int = 14) -> pd.Series:
    tr1 = high - low
    tr2 = abs(high - close.shift())
    tr3 = abs(low - close.shift())
    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
    return tr.rolling(window=min(window, len(close)), min_periods=1).mean()

================
File: app/services/llm_integration.py
================
from openai import OpenAI
import logging
import os
from dotenv import load_dotenv
import re
from app.services.technical_indicators import calculate_rsi, calculate_sma, calculate_ema, calculate_macd, calculate_bollinger_bands, calculate_atr

load_dotenv()

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

logger = logging.getLogger(__name__)


class GPT4Processor:
    def __init__(self):
        logger.info("Initializing GPT4Processor")

    def process_text(self, messages):
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                max_tokens=700,
                n=1,
                stop=None,
                temperature=0.7
            )
            generated_text = response.choices[0].message.content.strip()
            return generated_text
        except Exception as e:
            logger.error(f"Error processing text with GPT-4: {str(e)}")
            raise

    def analyze_news_sentiment(self, news_data):
        logger.info("Analyzing sentiment and generating explanation for news data")
        combined_text = "\n".join([f"Title: {article['title']}\nSummary: {article['summary']}" for article in news_data])
        messages = [
            {"role": "system", "content": "You are a financial analyst. Analyze the sentiment of the following news articles and provide a comprehensive analysis. Your very first line should just be the sentiment score, a value between -1 (very negative) and 1 (very positive). Then starting from a new line, provide a brief explanation for the sentiment score, followed by a bullet-point summary of key insights from the news."},
            {"role": "user", "content": combined_text},
        ]
        response = self.process_text(messages)
        
        # Parse the response to extract sentiment score and explanation
        lines = response.split('\n')
        sentiment_score = float(re.search(r"-?\d+\.\d+", lines[0].strip()).group())
        explanation = '\n'.join(lines[1:])
        
        logger.info(f"Sentiment analysis result: {sentiment_score}")
        logger.info(f"Sentiment explanation: {explanation}")
        
        return sentiment_score, explanation

    async def run_arima_strategy(self, stock_code, time_frame='1year'):
        from backend.scripts.arima_strategy import run_strategy_test
        arima_results = await run_strategy_test(stock_code, time_frame=time_frame)
        return arima_results

    async def final_analysis(self, stock_code, news_sentiment, sentiment_explanation, lstm_prediction, technical_indicators, historical_data):
        logger.info("Generating final analysis")
        
        # Run ARIMA strategy
        arima_results = await self.run_arima_strategy(stock_code)
        
        messages = [
            {"role": "system", "content": "You are a financial analyst for the Indian Stock Market. Based on the given information, provide a detailed analysis of the stock's performance and future outlook. Include key factors influencing your decision and recommend whether a shareholder should buy, hold, or sell. Structure your response with clear sections and bullet points for easy readability."},
            {"role": "user", "content": f"""
            Stock Code: {stock_code}
            News Sentiment Score: {news_sentiment}
            News Sentiment Analysis: {sentiment_explanation}
            LSTM Model Prediction: {lstm_prediction}
            Historical Data: {historical_data}
            Technical Indicators:
            - SMA 20: {technical_indicators['sma_20']}
            - EMA 50: {technical_indicators['ema_50']}
            - RSI: {technical_indicators['rsi']}
            - MACD: {technical_indicators['macd']}
            - ATR: {technical_indicators['atr']}
            ARIMA Strategy Results:
            - Total Return: {arima_results['total_return']}
            - Annualized Return: {arima_results['annualized_return']}
            - Sharpe Ratio: {arima_results['sharpe_ratio']}
            - Max Drawdown: {arima_results['max_drawdown']}
            - Win Rate: {arima_results['win_rate']}
            """},
        ]
        analysis = self.process_text(messages)
        logger.info(f"Final analysis: {analysis}")
        return analysis

    async def process_news_data(self, news_data, lstm_prediction, symbol, historical_data):
        sentiment_score, explanation = self.analyze_news_sentiment(news_data)
        technical_indicators = await self.fetch_technical_indicators(symbol)
        final_analysis = await self.final_analysis(
            symbol,
            sentiment_score,
            explanation,
            lstm_prediction,
            technical_indicators,
            historical_data
        )
        return {
            'sentiment': sentiment_score,
            'explanation': explanation,
            'analysis': final_analysis
        }

    async def fetch_technical_indicators(self, symbol):
        from app.services.data_collection import fetch_historical_data
        historical_data = await fetch_historical_data(symbol, '1year')
        if historical_data is None:
            logger.error(f"No historical data found for {symbol}")
            return None

        close_prices = historical_data['close']
        high_prices = historical_data['high']
        low_prices = historical_data['low']

        indicators = {
            'sma_20': calculate_sma(close_prices, 20).iloc[-1],
            'ema_50': calculate_ema(close_prices, 50).iloc[-1],
            'rsi': calculate_rsi(close_prices).iloc[-1],
            'macd': calculate_macd(close_prices)[0].iloc[-1],  # MACD line
            'atr': calculate_atr(high_prices, low_prices, close_prices).iloc[-1]
        }

        return indicators

================
File: app/services/zerodha_service.py
================
from kiteconnect import KiteConnect
import os
import logging
from functools import wraps
import traceback
import time
import dotenv

dotenv.load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class RateLimiter:
    def __init__(self, max_calls, period):
        self.max_calls = max_calls
        self.period = period
        self.calls = []

    def __call__(self, func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            now = time.time()
            self.calls = [call for call in self.calls if call > now - self.period]
            if len(self.calls) >= self.max_calls:
                sleep_time = self.calls[0] - (now - self.period)
                time.sleep(max(0, sleep_time))
            self.calls.append(time.time())
            return func(*args, **kwargs)

        return wrapper


rate_limiter = RateLimiter(max_calls=5, period=1)  # 5 calls per second


class ZerodhaService:
    def __init__(self):
        self.api_key = os.getenv("ZERODHA_API_KEY")
        self.api_secret = os.getenv("ZERODHA_API_SECRET")
        self.access_token = os.getenv("ZERODHA_ACCESS_TOKEN")
        self.kite = KiteConnect(api_key=self.api_key)
        if self.access_token:
            self.kite.set_access_token(self.access_token)

    @RateLimiter(max_calls=5, period=1)
    def get_quote(self, instruments):
        try:
            return self.kite.quote(instruments)
        except Exception as e:
            logger.error(f"Error fetching quotes: {str(e)}")
            return None

    def get_login_url(self):
        return self.kite.login_url()

    def generate_session(self, request_token):
        data = self.kite.generate_session(request_token, api_secret=self.api_secret)
        self.access_token = data["access_token"]
        self.kite.set_access_token(self.access_token)
        return self.access_token

    def set_access_token(self, access_token):
        self.access_token = access_token
        self.kite.set_access_token(self.access_token)
        # Optionally, update the .env file
        dotenv.set_key(dotenv.find_dotenv(), "ZERODHA_ACCESS_TOKEN", access_token)

    def get_instrument_token(self, exchange, symbol):
        try:
            instruments = self.kite.instruments(exchange)
            for instrument in instruments:
                if instrument['tradingsymbol'] == symbol or instrument['name'].upper() == symbol.upper():
                    return instrument['instrument_token']
            return None
        except Exception as e:
            logger.error(f"Error fetching instrument token: {str(e)}")
            return None

    @rate_limiter
    def get_historical_data(self, instrument_token, from_date, to_date, interval="day", continuous=0, oi=0):
        """
        Fetch historical data for the given instrument.

        :param instrument_token: The token of the instrument (e.g., 738561 for RELIANCE)
        :param from_date: The start date in yyyy-mm-dd hh:mm:ss format
        :param to_date: The end date in yyyy-mm-dd hh:mm:ss format
        :param interval: The interval of the candle (e.g., minute, day, etc.)
        :param continuous: Pass 1 for continuous data (for futures/options)
        :param oi: Pass 1 to get Open Interest (OI) data
        :return: List of candles or None if an error occurred
        """
        try:
            logger.info(f"Fetching historical data for token {instrument_token} from {from_date} to {to_date}")

            data = self.kite.historical_data(
                instrument_token=instrument_token,
                from_date=from_date,
                to_date=to_date,
                interval=interval,
                continuous=continuous,
                oi=oi
            )

            if not data:
                logger.warning(f"No historical data returned for token {instrument_token}")
            else:
                logger.info(f"Fetched {len(data)} candles")
            return data
        except Exception as e:
            logger.error(f"Error fetching historical data: {str(e)}")
            logger.error(traceback.format_exc())
            return None

    @rate_limiter
    def get_holdings(self):
        try:
            return self.kite.holdings()
        except Exception as e:
            logger.error(f"Error fetching holdings: {str(e)}")
            return None

    @rate_limiter
    def get_positions(self):
        try:
            return self.kite.positions()
        except Exception as e:
            logger.error(f"Error fetching positions: {str(e)}")
            return None

    @rate_limiter
    def place_order(self, exchange, tradingsymbol, transaction_type, quantity, price=None, product=None,
                    order_type=None):
        try:
            return self.kite.place_order(
                variety=self.kite.VARIETY_REGULAR,
                exchange=exchange,
                tradingsymbol=tradingsymbol,
                transaction_type=transaction_type,
                quantity=quantity,
                price=price,
                product=product or self.kite.PRODUCT_CNC,
                order_type=order_type or self.kite.ORDER_TYPE_MARKET
            )
        except Exception as e:
            logger.error(f"Error placing order: {str(e)}")
            return None

    @rate_limiter
    def get_orders(self):
        try:
            return self.kite.orders()
        except Exception as e:
            logger.error(f"Error fetching orders: {str(e)}")
            return None

================
File: app/services/data_collection.py
================
from .zerodha_service import ZerodhaService
import pandas as pd
import os
from datetime import datetime, timedelta
import traceback
import logging
from .technical_indicators import calculate_rsi, calculate_sma, calculate_ema, calculate_macd, calculate_bollinger_bands, calculate_atr
from ..db.influx_writer import write_stock_data_to_influxdb
from ..db.influx_client import get_influxdb_client
from .llm_integration import GPT4Processor
import aiohttp
from bs4 import BeautifulSoup
import yfinance as yf
import asyncio
import urllib.parse
import xml.etree.ElementTree as ET

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

zerodha_service = ZerodhaService()
client = get_influxdb_client()

llm_processor = GPT4Processor()

async def fetch_historical_data(scrip_code: str, time_frame: str = '1year'):
    try:
        end_date = datetime.now()
        
        if time_frame == '1month':
            start_date = end_date - timedelta(days=30)
        elif time_frame == '3months':
            start_date = end_date - timedelta(days=90)
        elif time_frame == '1year':
            start_date = end_date - timedelta(days=365)
        elif time_frame == '5years':
            start_date = end_date - timedelta(days=1825)
        else:
            raise ValueError(f"Invalid time frame: {time_frame}")
        
        logger.info(f"Fetching instrument token for {scrip_code}")
        instrument_token = zerodha_service.get_instrument_token("BSE", scrip_code)
        
        if not instrument_token:
            logger.error(f"No instrument token found for {scrip_code}")
            return None
        
        logger.info(f"Fetching historical data for {scrip_code} from {start_date} to {end_date}")
        data = zerodha_service.get_historical_data(
            instrument_token,
            start_date,
            end_date,
            "day"
        )
        
        if not data:
            logger.warning(f"No data found for {scrip_code}. It may be delisted.")
            return None

        df = pd.DataFrame(data)
        logger.info(f"Columns in the dataframe: {df.columns}")
        logger.info(f"Successfully fetched {len(df)} data points for {scrip_code}")
        return df
    except Exception as e:
        logger.error(f"Error fetching historical data for {scrip_code}: {str(e)}")
        logger.error(traceback.format_exc())
        return None

async def fetch_process_store_data(scrip_code: str, time_frame: str = '1year'):
    df = await fetch_historical_data(scrip_code, time_frame)
    if df is not None:
        # Calculate technical indicators
        df['RSI'] = calculate_rsi(df['close'])
        df['SMA_20'] = calculate_sma(df['close'], 20)
        df['EMA_50'] = calculate_ema(df['close'], 50)
        df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = calculate_macd(df['close'])
        df['Bollinger_Upper'], df['Bollinger_Middle'], df['Bollinger_Lower'] = calculate_bollinger_bands(df['close'])
        df['ATR'] = calculate_atr(df['high'], df['low'], df['close'])

        # Fetch and process news data
        news_data = await fetch_news_data(scrip_code)
        news_analysis = await process_news_data(news_data, df['close'].tolist())
        
        # Add news analysis to the dataframe
        df['News_Sentiment'] = news_analysis['sentiment']
        df['News_Explanation'] = news_analysis['explanation']
        df['News_Summary'] = news_analysis['analysis']

        # Write to InfluxDB
        write_stock_data_to_influxdb(df, scrip_code)
        logger.info(f"Data for {scrip_code} processed and stored in InfluxDB.")
    else:
        logger.error(f"Skipping {scrip_code} due to missing data.")


async def fetch_news_data(scrip_code: str, limit: int = 5):
    try:
        base_url = "https://news.google.com/rss/search"
        query = urllib.parse.urlencode({"q": f"{scrip_code} stock"})
        search_url = f"{base_url}?{query}&hl=en-IN&gl=IN&ceid=IN:en"

        async with aiohttp.ClientSession() as session:
            async with session.get(search_url) as response:
                if response.status != 200:
                    logger.error(f"Failed to fetch news for {scrip_code}. Status code: {response.status}")
                    return []
                
                xml_content = await response.text()
                root = ET.fromstring(xml_content)

                articles = []
                for item in root.findall('.//item')[:limit]:
                    title = item.find('title').text
                    link = item.find('link').text
                    pub_date = item.find('pubDate').text
                    description = item.find('description').text

                    articles.append({
                        'title': title,
                        'link': link,
                        'published_time': pub_date,
                        'summary': description[:200] + '...' if len(description) > 200 else description
                    })
                
                return articles
    except Exception as e:
        logger.error(f"Error fetching news for {scrip_code}: {str(e)}")
        return []


async def process_news_data(news_data, lstm_prediction, symbol, historical_data):
    if not news_data or len(news_data) == 0:
        logger.warning("No news data to process, defaulting sentiment to 0.")

        # Default values if no news data is present
        sentiment_score = 0
        explanation = 'No News Data found'

        # Fetch technical indicators even without news data
        technical_indicators = await fetch_technical_indicators(symbol)

        # Perform final analysis based on other information
        final_analysis = llm_processor.final_analysis(sentiment_score, explanation, lstm_prediction,
                                                      technical_indicators, historical_data)

        result = {
            'sentiment': sentiment_score,
            'explanation': explanation,
            'analysis': final_analysis
        }
        logger.info(f"Final sentiment analysis result without news data: {result}")
        return result

    logger.info(f"Processing {len(news_data)} news articles")

    # Analyze sentiment using the news data
    sentiment_score, explanation = llm_processor.analyze_news_sentiment(news_data)

    # Fetch technical indicators using the symbol from the news data
    technical_indicators = await fetch_technical_indicators(symbol)

    # Perform final analysis combining sentiment score, explanation, LSTM prediction, and technical indicators
    final_analysis = llm_processor.final_analysis(sentiment_score, explanation, lstm_prediction, technical_indicators, historical_data)

    result = {
        'sentiment': sentiment_score,
        'explanation': explanation,
        'analysis': final_analysis
    }

    logger.info(f"Final sentiment analysis result: {result}")
    return result


async def fetch_technical_indicators(symbol):
    historical_data = await fetch_historical_data(symbol, '1year')
    if historical_data is None:
        logger.error(f"No historical data found for {symbol}")
        return None

    close_prices = historical_data['close']
    high_prices = historical_data['high']
    low_prices = historical_data['low']

    indicators = {
        'sma_20': calculate_sma(close_prices, 20).iloc[-1],
        'ema_50': calculate_ema(close_prices, 50).iloc[-1],
        'rsi': calculate_rsi(close_prices).iloc[-1],
        'macd': calculate_macd(close_prices)[0].iloc[-1],  # MACD line
        'atr': calculate_atr(high_prices, low_prices, close_prices).iloc[-1]
    }

    return indicators


async def update_influxdb_with_latest_data(scrip_code: str):
    try:
        query_api = client.query_api()
        query = f'''
        from(bucket: "stock_data")
          |> range(start: 0)
          |> filter(fn: (r) => r._measurement == "stock_data" and r.symbol == "{scrip_code}")
          |> keep(columns: ["_time"])
          |> sort(columns: ["_time"], desc: true)
          |> limit(n: 1)
        '''
        result = query_api.query(org="my_org", query=query)
        if not result:
            logger.warning(f"No existing data found for {scrip_code} in InfluxDB.")
            await fetch_process_store_data(scrip_code, '5years')
            return

        last_date = result[0].records[0].get_time()
        start_date = last_date + timedelta(days=1)
        end_date = datetime.now()

        logger.info(f"Fetching latest data for {scrip_code} from {start_date} to {end_date}")
        instrument_token = zerodha_service.get_instrument_token("BSE", scrip_code)
        if not instrument_token:
            logger.error(f"No instrument token found for {scrip_code}")
            return

        data = zerodha_service.get_historical_data(
            instrument_token,
            start_date,
            end_date,
            "day"
        )

        if not data:
            logger.warning(f"No new data found for {scrip_code}.")
            return

        df = pd.DataFrame(data)
        logger.info(f"Columns in the dataframe: {df.columns}")
        logger.info(f"Successfully fetched {len(df)} new data points for {scrip_code}")

        # Calculate technical indicators
        df['RSI'] = calculate_rsi(df['close'])
        df['SMA_20'] = calculate_sma(df['close'], 20)
        df['EMA_50'] = calculate_ema(df['close'], 50)
        df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = calculate_macd(df['close'])
        df['Bollinger_Upper'], df['Bollinger_Middle'], df['Bollinger_Lower'] = calculate_bollinger_bands(df['close'])
        df['ATR'] = calculate_atr(df['high'], df['low'], df['close'])

        # Write to InfluxDB
        write_stock_data_to_influxdb(df, scrip_code)
        logger.info(f"New data for {scrip_code} processed and stored in InfluxDB.")
    except Exception as e:
        logger.error(f"Error updating data for {scrip_code}: {str(e)}")
        logger.error(traceback.format_exc())

================
File: app/services/data_fetcher.py
================
from .zerodha_service import ZerodhaService
from typing import Dict, Any, List

zerodha_service = ZerodhaService()

async def fetch_bse_data(scrip_code: str) -> Dict[str, Any]:
    return zerodha_service.get_quote(f"BSE:{scrip_code}")

async def fetch_top_bse_stocks(limit: int = 100) -> List[str]:
    # This functionality is not directly available in Zerodha API
    # You might need to implement a custom solution or use another data source
    raise NotImplementedError("Fetching top BSE stocks is not implemented with Zerodha API")

async def fetch_multiple_bse_stocks(scrip_codes: List[str] = None) -> Dict[str, Any]:
    if not scrip_codes:
        # You might need to implement a custom solution to get top companies
        raise NotImplementedError("Fetching top companies is not implemented with Zerodha API")

    return zerodha_service.get_quote([f"{code}" for code in scrip_codes])

async def fetch_top_gainers() -> List[Dict[str, Any]]:
    # This functionality is not directly available in Zerodha API
    # You might need to implement a custom solution or use another data source
    raise NotImplementedError("Fetching top gainers is not implemented with Zerodha API")

async def fetch_top_losers() -> List[Dict[str, Any]]:
    # This functionality is not directly available in Zerodha API
    # You might need to implement a custom solution or use another data source
    raise NotImplementedError("Fetching top losers is not implemented with Zerodha API")

async def fetch_indices(category: str = 'market_cap/broad') -> Dict[str, Any]:
    # You might need to adjust this based on available Zerodha API endpoints
    raise NotImplementedError("Fetching indices is not implemented with Zerodha API")

================
File: tests/test_zerodha_service.py
================
import unittest
from unittest.mock import patch, MagicMock
import sys
import os
import time

# Add the parent directory of 'tests' to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app.services.zerodha_service import ZerodhaService, RateLimiter


class TestZerodhaService(unittest.TestCase):

    @patch('app.services.zerodha_service.os.getenv')
    @patch('app.services.zerodha_service.KiteConnect')
    def setUp(self, mock_kite, mock_getenv):
        # Mock environment variables
        mock_getenv.side_effect = lambda key: {'ZERODHA_API_KEY': 'mock_api_key',
                                               'ZERODHA_API_SECRET': 'mock_api_secret'}.get(key)

        self.mock_kite = mock_kite
        self.zerodha_service = ZerodhaService()
        self.zerodha_service.kite = self.mock_kite.return_value

    def test_get_quote(self):
        self.mock_kite.return_value.quote.return_value = {"BSE:500325": {"last_price": 100}}
        result = self.zerodha_service.get_quote(["BSE:500325"])
        self.assertEqual(result, {"BSE:500325": {"last_price": 100}})

    def test_get_quote_error(self):
        self.mock_kite.return_value.quote.side_effect = Exception("API Error")
        result = self.zerodha_service.get_quote(["BSE:500325"])
        self.assertIsNone(result)

    def test_rate_limiter(self):
        @RateLimiter(max_calls=2, period=1)
        def test_function():
            pass

        start_time = time.time()
        test_function()
        test_function()
        test_function()
        end_time = time.time()
        self.assertGreaterEqual(end_time - start_time, 1)


if __name__ == '__main__':
    unittest.main()

================
File: tests/conftest.py
================
import sys
import os

# Add the parent directory of 'tests' to the Python path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

================
File: tests/test_update_data.py
================
import os
import pandas as pd
from datetime import datetime, timedelta
import logging
import asyncio
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from app.services.zerodha_service import ZerodhaService
from app.db.influx_writer import write_stock_data_to_influxdb
from app.db.influx_client import get_influxdb_client
from app.services.technical_indicators import calculate_rsi, calculate_sma, calculate_ema, calculate_macd, calculate_bollinger_bands, calculate_atr

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

zerodha_service = ZerodhaService()
client = get_influxdb_client()

async def fetch_historical_data(scrip_code: str, time_frame: str = '1year'):
    try:
        end_date = datetime.now()
        
        if time_frame == '1month':
            start_date = end_date - timedelta(days=30)
        elif time_frame == '3months':
            start_date = end_date - timedelta(days=90)
        elif time_frame == '1year':
            start_date = end_date - timedelta(days=365)
        elif time_frame == '5years':
            start_date = end_date - timedelta(days=1825)
        else:
            raise ValueError(f"Invalid time frame: {time_frame}")
        
        logger.info(f"Fetching instrument token for {scrip_code}")
        instrument_token = zerodha_service.get_instrument_token("BSE", scrip_code)
        
        if not instrument_token:
            logger.error(f"No instrument token found for {scrip_code}")
            return None
        
        logger.info(f"Fetching historical data for {scrip_code} from {start_date} to {end_date}")
        data = zerodha_service.get_historical_data(
            instrument_token,
            start_date,
            end_date,
            "day"
        )
        
        if not data:
            logger.warning(f"No data found for {scrip_code}. It may be delisted.")
            return None

        df = pd.DataFrame(data)
        logger.info(f"Columns in the dataframe: {df.columns}")
        logger.info(f"Successfully fetched {len(df)} data points for {scrip_code}")
        return df
    except Exception as e:
        logger.error(f"Error fetching historical data for {scrip_code}: {str(e)}")
        return None

async def add_data_to_influxdb(scrip_code: str, time_frame: str = '1year'):
    df = await fetch_historical_data(scrip_code, time_frame)
    if df is not None:
        # Calculate technical indicators
        df['RSI'] = calculate_rsi(df['close'])
        df['SMA_20'] = calculate_sma(df['close'], 20)
        df['EMA_50'] = calculate_ema(df['close'], 50)
        df['MACD'], df['MACD_Signal'], df['MACD_Hist'] = calculate_macd(df['close'])
        df['Bollinger_Upper'], df['Bollinger_Middle'], df['Bollinger_Lower'] = calculate_bollinger_bands(df['close'])
        df['ATR'] = calculate_atr(df['high'], df['low'], df['close'])

        # Write to InfluxDB
        write_stock_data_to_influxdb(df, scrip_code)
        logger.info(f"Data for {scrip_code} processed and stored in InfluxDB.")
    else:
        logger.error(f"Skipping {scrip_code} due to missing data.")

if __name__ == "__main__":
    scrip_code = "RELIANCE"  # Change this to the stock code you want to add
    asyncio.run(add_data_to_influxdb(scrip_code))

================
File: tests/test_zerodha_connection.py
================
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from app.services.zerodha_service import ZerodhaService
from dotenv import load_dotenv
import logging

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_zerodha_connection():
    zerodha_service = ZerodhaService()
    
    # Test login URL
    login_url = zerodha_service.get_login_url()
    logger.info(f"Login URL: {login_url}")
    
    # Test fetching a quote
    symbol = "RELIANCE"  # You can change this to any stock symbol
    quote = zerodha_service.get_quote(f"BSE:{symbol}")
    if quote:
        logger.info(f"Quote for {symbol}: {quote}")
    else:
        logger.error(f"Failed to fetch quote for {symbol}")
    
    # Test fetching instrument token
    instrument_token = zerodha_service.get_instrument_token("BSE", symbol)
    if instrument_token:
        logger.info(f"Instrument token for {symbol}: {instrument_token}")
    else:
        logger.error(f"Failed to fetch instrument token for {symbol}")

if __name__ == "__main__":
    test_zerodha_connection()

================
File: tests/query_influxdb.py
================
from influxdb_client import InfluxDBClient
import os
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# InfluxDB connection details
token = os.environ.get("INFLUXDB_TOKEN")
org = "my_org"
bucket = "stock_data"
url = "http://localhost:8086"

def query_influxdb(scrip_code):
    client = InfluxDBClient(url=url, token=token, org=org)
    query_api = client.query_api()

    query = f'''
    from(bucket: "{bucket}")
      |> range(start: -1y)
      |> filter(fn: (r) => r._measurement == "stock_data" and r.symbol == "{scrip_code}")
      |> limit(n: 10)
    '''
    
    logger.info(f"Executing query: {query}")
    
    try:
        result = query_api.query(org=org, query=query)
        logger.info(f"Query result for {scrip_code}:")
        for table in result:
            for record in table.records:
                logger.info(f"Time: {record.get_time()}, Fields: {record.values}")
    except Exception as e:
        logger.error(f"Error querying InfluxDB: {e}")

if __name__ == "__main__":
    scrip_code = "RELIANCE"  # Change this to the stock code you want to query
    query_influxdb(scrip_code)

================
File: tests/test_historical.py
================
import os
from datetime import datetime, timedelta
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from app.services.zerodha_service import ZerodhaService
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

zerodha_service = ZerodhaService()

def test_fetch_historical_data(scrip_code: str):
    try:
        instrument_token = zerodha_service.get_instrument_token("BSE", scrip_code)
        if not instrument_token:
            logger.error(f"No instrument token found for {scrip_code}")
            return

        end_date = datetime.now()
        start_date = end_date - timedelta(days=365 * 10)  # Attempt to fetch 10 years of data

        logger.info(f"Fetching historical data for {scrip_code} from {start_date} to {end_date}")
        data = zerodha_service.get_historical_data(
            instrument_token,
            start_date,
            end_date,
            "day"
        )

        if not data:
            logger.warning(f"No data found for {scrip_code}. It may be delisted or data may not be available for the entire period.")
            return

        logger.info(f"Successfully fetched {len(data)} data points for {scrip_code}")
    except Exception as e:
        logger.error(f"Error fetching historical data for {scrip_code}: {str(e)}")

if __name__ == "__main__":
    scrip_code = "RELIANCE"  # Change this to the stock code you want to test
    test_fetch_historical_data(scrip_code)

================
File: tests/test_api_endpoints.py
================
import requests
import json

BASE_URL = "http://localhost:8000"

def test_root():
    response = requests.get(f"{BASE_URL}/")
    print("Root Endpoint:")
    print(f"Status Code: {response.status_code}")
    print(f"Response: {response.json()}")
    print()

def test_historical_data(symbol="RELIANCE", days=30):
    response = requests.get(f"{BASE_URL}/api/stocks/{symbol}/data?days={days}")
    print(f"Historical Data for {symbol}:")
    print(f"Status Code: {response.status_code}")
    if response.status_code == 200:
        data = response.json()
        print(f"Number of data points: {len(data['data'])}")
        if data['data']:
            print(f"First data point: {json.dumps(data['data'][0], indent=2)}")
        else:
            print("No data points available")
    else:
        print(f"Error: {response.text}")
    print()

def test_technical_indicators(symbol="RELIANCE", days=30):
    response = requests.get(f"{BASE_URL}/api/stocks/{symbol}/indicators?days={days}")
    print(f"Technical Indicators for {symbol}:")
    print(f"Status Code: {response.status_code}")
    if response.status_code == 200:
        data = response.json()
        print(f"Indicators: {json.dumps(data, indent=2)}")
    else:
        print(f"Error: {response.text}")
    print()

def test_realtime_data(symbol="RELIANCE"):
    response = requests.get(f"{BASE_URL}/api/stocks/{symbol}/realtime")
    print(f"Realtime Data for {symbol}:")
    print(f"Status Code: {response.status_code}")
    if response.status_code == 200:
        data = response.json()
        print(f"Realtime data: {json.dumps(data, indent=2)}")
    else:
        print(f"Error: {response.text}")
    print()

def test_market_overview(limit=5):
    response = requests.get(f"{BASE_URL}/api/market/overview?limit={limit}")
    print(f"Market Overview (Top {limit} stocks):")
    print(f"Status Code: {response.status_code}")
    if response.status_code == 200:
        data = response.json()
        print(f"Overview: {json.dumps(data, indent=2)}")
    else:
        print(f"Error: {response.text}")
    print()

def test_compare_stocks(symbols="RELIANCE,TCS", days=30):
    response = requests.get(f"{BASE_URL}/api/stocks/compare?symbols={symbols}&days={days}")
    print(f"Compare Stocks ({symbols}):")
    print(f"Status Code: {response.status_code}")
    if response.status_code == 200:
        data = response.json()
        print(f"Comparison data: {json.dumps(data, indent=2)}")
    else:
        print(f"Error: {response.text}")
    print()

if __name__ == "__main__":
    test_root()
    test_historical_data()
    test_technical_indicators()
    test_realtime_data()
    test_market_overview()
    test_compare_stocks()

================
File: scripts/get_stocks_list.py
================
from kiteconnect import KiteConnect
import pandas as pd
import os
import dotenv

dotenv.load_dotenv()

# Initialize Kite Connect
api_key = os.getenv("ZERODHA_API_KEY")
access_token = os.getenv("ZERODHA_ACCESS_TOKEN")
kite = KiteConnect(api_key=api_key)
kite.set_access_token(access_token)

# Fetch all instruments
instruments = kite.instruments()

# Convert to DataFrame
df = pd.DataFrame(instruments)

# Filter for Indian stocks (NSE and BSE)
indian_stocks = df[(df['exchange'].isin(['NSE', 'BSE'])) & (df['instrument_type'] == 'EQ')]

# Select relevant columns
columns_to_keep = ['tradingsymbol', 'name', 'exchange', 'instrument_token', 'exchange_token']
indian_stocks = indian_stocks[columns_to_keep]

# Save to CSV
csv_filename = 'indian_stocks.csv'
indian_stocks.to_csv(csv_filename, index=False)

print(f"CSV file '{csv_filename}' has been created with {len(indian_stocks)} Indian stocks.")

================
File: scripts/model_comparison.py
================
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from app.models.lstm_model import LSTMStockPredictor
from app.models.arima_model import ARIMAStockPredictor
from app.services.data_collection import fetch_historical_data
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
import asyncio
import joblib
import logging
from app.models.backtesting import backtest_model as backtesting_model

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Constants
INVESTMENT_AMOUNT = 100000  # Initial investment amount
TRANSACTION_COST = 0.001  # 0.1% transaction cost
THRESHOLD = 0.01  # 1% threshold for buy/sell decisions
MAX_STOCKS = 10
BATCH_SIZE = 3
RATE_LIMIT_DELAY = 1  # 1 second delay between batches


async def backtest_model(model_type, stock_code, historical_data):
    if len(historical_data) < 30:
        logger.warning(f"Insufficient data for {stock_code} to perform backtesting. Skipping.")
        return None

    try:
        result = backtesting_model(stock_code, historical_data, model_type, 
                                   investment_amount=INVESTMENT_AMOUNT, 
                                   threshold=THRESHOLD, 
                                   transaction_cost=TRANSACTION_COST)
        return result
    except Exception as e:
        logger.error(f"Error in {model_type} backtesting for {stock_code}: {str(e)}")
        return None


async def process_batch(batch, time_frame):
    results = []
    insufficient_data = []
    for stock_code in batch:
        logger.info(f"Backtesting {stock_code}")
        historical_data = await fetch_historical_data(stock_code, time_frame)
        if historical_data is None or len(historical_data) < 30:  # Reduced minimum data points
            logger.warning(f"Insufficient data available for {stock_code}. Got {len(historical_data) if historical_data is not None else 0} data points, need at least 30.")
            insufficient_data.append(stock_code)
            continue

        lstm_result = await backtest_model('LSTM', stock_code, historical_data)
        arima_result = await backtest_model('ARIMA', stock_code, historical_data)

        if lstm_result and arima_result:
            results.append({
                'stock_code': stock_code,
                'lstm_result': lstm_result,
                'arima_result': arima_result
            })
        else:
            logger.warning(f"Skipping {stock_code} due to backtesting failure")

    return results, insufficient_data


async def compare_models(stock_codes, time_frame='3months'):
    all_results = []
    all_insufficient_data = []

    for i in range(0, len(stock_codes), BATCH_SIZE):
        batch = stock_codes[i:i+BATCH_SIZE]
        results, insufficient_data = await process_batch(batch, time_frame)
        all_results.extend(results)
        all_insufficient_data.extend(insufficient_data)

        if i + BATCH_SIZE < len(stock_codes):
            logger.info(f"Waiting {RATE_LIMIT_DELAY} second(s) before processing next batch...")
            await asyncio.sleep(RATE_LIMIT_DELAY)

    return all_results, all_insufficient_data


def print_summary(results, insufficient_data):
    print("\nModel Comparison Summary:")
    print("=" * 50)

    if not results:
        print("No stocks had sufficient data for backtesting.")
        return

    lstm_results = []
    arima_results = []
    lstm_profitable = 0
    arima_profitable = 0

    for result in results:
        print(f"\nStock: {result['stock_code']}")
        print("-" * 30)
        for model in ['lstm_result', 'arima_result']:
            model_result = result[model]
            print(f"{model_result['model_type']} Model:")
            print(f"  Initial Investment: ${model_result['initial_investment']:.2f}")
            print(f"  Final Portfolio Value: ${model_result['final_portfolio_value']:.2f}")
            print(f"  Total Return: {model_result['total_return_percentage']:.2f}%")
            print(f"  Number of Trades: {model_result['number_of_trades']}")
            
            if model == 'lstm_result':
                lstm_results.append((result['stock_code'], model_result['total_return_percentage']))
                if model_result['total_return_percentage'] > 0:
                    lstm_profitable += 1
            else:
                arima_results.append((result['stock_code'], model_result['total_return_percentage']))
                if model_result['total_return_percentage'] > 0:
                    arima_profitable += 1
        print()

    # Calculate overall performance
    lstm_total_return = sum(r[1] for r in lstm_results)
    arima_total_return = sum(r[1] for r in arima_results)
    lstm_avg_return = lstm_total_return / len(results)
    arima_avg_return = arima_total_return / len(results)

    print("Overall Performance:")
    print(f"LSTM Average Return: {lstm_avg_return:.2f}%")
    print(f"ARIMA Average Return: {arima_avg_return:.2f}%")

    # Best and worst performing stocks
    lstm_results.sort(key=lambda x: x[1], reverse=True)
    arima_results.sort(key=lambda x: x[1], reverse=True)

    print("\nBest Performing Stocks:")
    print(f"LSTM: {lstm_results[0][0]} ({lstm_results[0][1]:.2f}%)")
    print(f"ARIMA: {arima_results[0][0]} ({arima_results[0][1]:.2f}%)")

    print("\nWorst Performing Stocks:")
    print(f"LSTM: {lstm_results[-1][0]} ({lstm_results[-1][1]:.2f}%)")
    print(f"ARIMA: {arima_results[-1][0]} ({arima_results[-1][1]:.2f}%)")

    print("\nProfitability:")
    print(f"LSTM: Profitable on {lstm_profitable} out of {len(results)} stocks ({lstm_profitable/len(results)*100:.2f}%)")
    print(f"ARIMA: Profitable on {arima_profitable} out of {len(results)} stocks ({arima_profitable/len(results)*100:.2f}%)")

    print("\nTop 5 Performing Stocks for Each Model:")
    print("LSTM:")
    for stock, return_percentage in lstm_results[:5]:
        print(f"  {stock}: {return_percentage:.2f}%")
    print("ARIMA:")
    for stock, return_percentage in arima_results[:5]:
        print(f"  {stock}: {return_percentage:.2f}%")

    print("\nBottom 5 Performing Stocks for Each Model:")
    print("LSTM:")
    for stock, return_percentage in reversed(lstm_results[-5:]):
        print(f"  {stock}: {return_percentage:.2f}%")
    print("ARIMA:")
    for stock, return_percentage in reversed(arima_results[-5:]):
        print(f"  {stock}: {return_percentage:.2f}%")

    print("\nStocks with insufficient data:")
    for stock in insufficient_data:
        print(f"- {stock}")

    print(f"\nTotal stocks processed: {len(results)}")
    print(f"Stocks with insufficient data: {len(insufficient_data)}")


def get_trained_stock_codes():
    model_dir = os.path.join(os.path.dirname(__file__), '..', 'models')
    stock_codes = set()
    for filename in os.listdir(model_dir):
        if filename.endswith('_lstm_model.h5'):
            stock_code = filename.split('_lstm_model.h5')[0]
            stock_codes.add(stock_code)
    return list(stock_codes)[:MAX_STOCKS]  # Limit to MAX_STOCKS


if __name__ == "__main__":
    stock_codes = get_trained_stock_codes()
    logger.info(f"Found {len(stock_codes)} trained models (limited to {MAX_STOCKS}): {', '.join(stock_codes)}")

    results, insufficient_data = asyncio.run(compare_models(stock_codes))
    print_summary(results, insufficient_data)

================
File: scripts/arima_strategy.py
================
import sys
import os
import asyncio
import pandas as pd
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import numpy as np
from statsmodels.tsa.arima.model import ARIMA
import quantstats as qs
import logging
import matplotlib.pyplot as plt
from app.services.data_collection import fetch_historical_data
import datetime

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ARIMAStrategy:
    def __init__(self, stock_code, initial_capital=100000, transaction_cost=0.001,
                 arima_order=(1, 1, 1), buy_threshold=0.01, sell_threshold=0.01):
        self.stock_code = stock_code
        self.initial_capital = initial_capital
        self.transaction_cost = transaction_cost
        self.arima_order = arima_order
        self.buy_threshold = buy_threshold
        self.sell_threshold = sell_threshold
        self.reset()

    def reset(self):
        self.cash = self.initial_capital
        self.shares = 0
        self.trades = []
        self.portfolio_values = []

    async def run_strategy(self, historical_data):
        self.reset()
        if historical_data is None or len(historical_data) < 30:
            logger.warning(f"Insufficient data for {self.stock_code}")
            return None

        close_prices = historical_data['close'].values
        dates = pd.to_datetime(historical_data.index)  # Ensure dates are datetime

        try:
            model = ARIMA(close_prices, order=self.arima_order)
            model_fit = model.fit()
        except Exception as e:
            logger.error(f"Error fitting ARIMA model for {self.stock_code}: {str(e)}")
            return None

        for i in range(1, len(close_prices)):
            current_price = close_prices[i]
            prev_price = close_prices[i - 1]
            prediction = model_fit.forecast(steps=1)[0]

            if prediction > prev_price * (1 + self.buy_threshold) and self.cash > current_price:
                shares_to_buy = (self.cash * (1 - self.transaction_cost)) // current_price
                if shares_to_buy > 0:
                    self.shares += shares_to_buy
                    self.cash -= shares_to_buy * current_price * (1 + self.transaction_cost)
                    self.trades.append(('buy', shares_to_buy, current_price, dates[i]))
            elif prediction < prev_price * (1 - self.sell_threshold) and self.shares > 0:
                self.cash += self.shares * current_price * (1 - self.transaction_cost)
                self.trades.append(('sell', self.shares, current_price, dates[i]))
                self.shares = 0

            portfolio_value = self.cash + self.shares * current_price
            self.portfolio_values.append(portfolio_value)

        return pd.Series(self.portfolio_values, index=dates[1:])


def evaluate_strategy(strategy, returns, strategy_name):
    report = generate_detailed_report(strategy, returns, strategy_name)

    # Print the report to console
    print(report)

    # Save the report to a text file
    with open(f'{strategy_name.lower().replace(" ", "_")}_report.txt', 'w') as f:
        f.write(report)


def generate_detailed_report(strategy, returns, strategy_name):
    cumulative_returns = (1 + returns).cumprod() - 1
    total_return = cumulative_returns.iloc[-1]

    # Check if the index is DatetimeIndex, if not, assume it's a range index
    if isinstance(returns.index, pd.DatetimeIndex):
        days = (returns.index[-1] - returns.index[0]).days
    else:
        days = len(returns)  # Assume each data point represents a day

    annualized_return = (1 + total_return) ** (365 / max(days, 1)) - 1 if days > 0 else 0
    risk_free_rate = 0.02
    sharpe_ratio = qs.stats.sharpe(returns, rf=risk_free_rate, periods=252)
    max_drawdown = qs.stats.max_drawdown(returns)
    win_rate = qs.stats.win_rate(returns)
    sortino_ratio = qs.stats.sortino(returns, rf=risk_free_rate, periods=252)

    report = f"""
{strategy_name} Detailed Report
==============================

Performance Metrics:
-------------------
Total Return: {total_return:.2%}
Annualized Return: {annualized_return:.2%}
Sharpe Ratio: {sharpe_ratio:.2f}
Sortino Ratio: {sortino_ratio:.2f}
Max Drawdown: {max_drawdown:.2%}
Win Rate: {win_rate:.2%}

Trade Details:
--------------
"""

    total_profit = 0
    for trade in strategy.trades:
        action, amount, price, date = trade
        if action == 'buy':
            trade_value = -amount * price
        else:  # sell
            trade_value = amount * price
        total_profit += trade_value
        report += f"{date} - {action.upper()}: {amount} shares at ${price:.2f} (Trade Value: ${trade_value:.2f})\n"

    report += f"\nTotal Profit: ${total_profit:.2f}"
    report += f"\nFinal Portfolio Value: ${strategy.portfolio_values[-1]:.2f}"
    report += f"\nOverall Profit %: {(strategy.portfolio_values[-1] / strategy.initial_capital - 1) * 100:.2f}%"

    return report


async def run_strategy_test(stock_code, arima_order=(1, 1, 1), buy_threshold=0.01, sell_threshold=0.01, time_frame='1year'):
    strategy = ARIMAStrategy(stock_code, arima_order=arima_order, buy_threshold=buy_threshold,
                             sell_threshold=sell_threshold)

    historical_data = await fetch_historical_data(stock_code, time_frame)

    if historical_data is not None and len(historical_data) >= 30:
        portfolio_values = await strategy.run_strategy(historical_data)

        if portfolio_values is not None and len(portfolio_values) > 1:
            returns = portfolio_values.pct_change().dropna()
            strategy_name = f"ARIMA Strategy ({stock_code}, order={arima_order}, buy={buy_threshold}, sell={sell_threshold}, time_frame={time_frame})"
            evaluate_strategy(strategy, returns, strategy_name)
        else:
            print(f"Insufficient trading data for {stock_code}")
    else:
        print(f"Insufficient historical data for {stock_code}")


async def main():
    stocks_to_test = ['GAEL', 'INFY', 'TCS']  # Add more stocks as needed
    arima_orders = [(1, 1, 1), (2, 1, 2), (1, 1, 2)]  # Different ARIMA orders to test
    thresholds = [(0.01, 0.01), (0.02, 0.02), (0.015, 0.01)]  # Different buy/sell thresholds
    time_frames = ['1month', '3months', '1year']  # Available time frames

    all_reports = ""

    for stock in stocks_to_test:
        for order in arima_orders:
            for buy_thresh, sell_thresh in thresholds:
                for time_frame in time_frames:
                    strategy = ARIMAStrategy(stock, arima_order=order, buy_threshold=buy_thresh,
                                             sell_threshold=sell_thresh)
                    historical_data = await fetch_historical_data(stock, time_frame)

                    if historical_data is not None and len(historical_data) >= 30:
                        portfolio_values = await strategy.run_strategy(historical_data)

                        if portfolio_values is not None and len(portfolio_values) > 1:
                            returns = portfolio_values.pct_change().dropna()
                            strategy_name = f"ARIMA Strategy ({stock}, order={order}, buy={buy_thresh}, sell={sell_thresh}, time_frame={time_frame})"
                            report = generate_detailed_report(strategy, returns, strategy_name)
                            all_reports += report + "\n\n" + "=" * 50 + "\n\n"
                        else:
                            all_reports += f"Insufficient trading data for {stock} with time frame {time_frame}\n\n"
                    else:
                        all_reports += f"Insufficient historical data for {stock} with time frame {time_frame}\n\n"

    # Save all reports to a single text file
    with open('all_arima_strategy_reports.txt', 'w') as f:
        f.write(all_reports)

    print("All reports have been saved to 'all_arima_strategy_reports.txt'")


if __name__ == "__main__":
    asyncio.run(main())
